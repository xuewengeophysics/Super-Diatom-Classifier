{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pickle\n",
    "from IPython.display import clear_output\n",
    "import PIL.Image\n",
    "import multiprocessing\n",
    "from lxml import etree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%run ./generator.ipynb\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../../utils/data_utils.ipynb\n",
    "%run ../../global_variables.ipynb\n",
    "\n",
    "TEST = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_maps():\n",
    "    images = []\n",
    "    for path in DATASET_PATH:\n",
    "        images.extend([f for f in listdir(path) if isfile(join(path, f))])\n",
    "\n",
    "    tmp_code = {}\n",
    "    i = 1\n",
    "    for file in images:\n",
    "        taxon = file.split('_')[1]\n",
    "        if not (taxon in tmp_code):\n",
    "            tmp_code[taxon] = i\n",
    "            i+=1\n",
    "\n",
    "    savePickle(tmp_code, os.path.join(SAVE_PATH, \"maps/multiclass_label_map.pickle\"))\n",
    "    binary_tmp_code = {}\n",
    "    binary_tmp_code[\"diatom\"] = 1\n",
    "    savePickle(binary_tmp_code, os.path.join(SAVE_PATH, \"maps/binary_label_map.pickle\"))\n",
    "    print(\"Binary and multiclass label maps saved successfully !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(lock, n_id, wnumber, train, n_images, verbose, folder):\n",
    "    # MAIN\n",
    "    ref = cv2.imread(os.path.join(DATASETS_ROOT,\"atlas/ref_img.png\"), cv2.IMREAD_GRAYSCALE)\n",
    "    diatom_images_train, diatom_images_val = split_img_list(list_images(DATASET_PATH, diatoms=True, ref=ref), PERCENTAGE_VAL)\n",
    "    debris_images_train, debris_images_val = split_img_list(list_images(DATASET_DUST_PATH, diatoms=False, ref=ref), PERCENTAGE_VAL)\n",
    "    if train:\n",
    "        images=[diatom_images_train, debris_images_train]\n",
    "    else:\n",
    "        images=[diatom_images_val, debris_images_val]\n",
    "    \n",
    "    print(\"Worker \", wnumber, \" ok!\")\n",
    "    np.random.seed(int(wnumber*1000))\n",
    "    lock.acquire()\n",
    "    wid = n_id.value\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        print(\"Worker \", wnumber, \": \", (wid+1),\"/\", n_images)\n",
    "    n_id.value += 1\n",
    "    lock.release()\n",
    "    while(wid<=n_images):\n",
    "        final_img, annotations = main_generator(images,\n",
    "                                          simple_angles = False, \n",
    "                                          size_px = 1000,\n",
    "                                          verbose=False, \n",
    "                                          overlapping=0.35, \n",
    "                                          n_diatoms=[9,12],\n",
    "                                          scale_diatoms=[3.3,3],                                          \n",
    "                                          n_dust=[20,40],\n",
    "                                          scale_dust=[3,4])\n",
    "        string_id = '{:05d}'.format(wid)\n",
    "        final_img = final_img[:,:,1]\n",
    "        \n",
    "        # Init xml tree\n",
    "        xml_root = etree.Element(\"annotation\")\n",
    "        etree.SubElement(xml_root, \"folder\").text = \"images\"\n",
    "        etree.SubElement(xml_root, \"filename\").text = string_id+\".png\"\n",
    "        source_xml = etree.SubElement(xml_root, \"source\")\n",
    "        etree.SubElement(source_xml, \"database\").text = DATASET_NAME\n",
    "        size_xml = etree.SubElement(xml_root, \"size\")\n",
    "        etree.SubElement(size_xml, \"width\").text = str(final_img.shape[0])\n",
    "        etree.SubElement(size_xml, \"height\").text = str(final_img.shape[1])\n",
    "        etree.SubElement(size_xml, \"depth\").text = str(1)\n",
    "        \n",
    "        path_img = \"images/\"+string_id+\".png\"\n",
    "        save_img(final_img, join(SAVE_PATH, folder, path_img));\n",
    "\n",
    "        ## Saving individual masks\n",
    "        taxon_n = {}\n",
    "        paths = []\n",
    "        for annotation in annotations:\n",
    "            taxon = annotation[\"taxon\"]\n",
    "            if taxon in taxon_n:\n",
    "                taxon_n[taxon] += 1\n",
    "            else:\n",
    "                taxon_n[taxon] = 0\n",
    "            path_mask = \"masks/\"+string_id+\"_\"+taxon+\"_\"+'{:03d}'.format(taxon_n[taxon])+\".png\"\n",
    "            # Saving mask\n",
    "            #img = PIL.Image.fromarray(annotation[\"patch_mask\"])\n",
    "            annotation.pop(\"patch_mask\")\n",
    "            #output = io.BytesIO()\n",
    "            check_dirs(join(SAVE_PATH, folder, path_mask))\n",
    "            #img.save(join(SAVE_PATH, path_mask), format='PNG')\n",
    "            annotation[\"mask_path\"] = path_mask\n",
    "            # Adding bounding box to xml\n",
    "            object_xml = etree.SubElement(xml_root, \"object\")\n",
    "            etree.SubElement(object_xml, \"name\").text = annotation[\"taxon\"]\n",
    "            bndbox_xml = etree.SubElement(object_xml, \"bndbox\")\n",
    "            etree.SubElement(bndbox_xml, \"xmin\").text = str(annotation[\"xmin\"])\n",
    "            etree.SubElement(bndbox_xml, \"ymin\").text = str(annotation[\"ymin\"])\n",
    "            etree.SubElement(bndbox_xml, \"xmax\").text = str(annotation[\"xmax\"])\n",
    "            etree.SubElement(bndbox_xml, \"ymax\").text = str(annotation[\"ymax\"])        \n",
    "        \n",
    "        # Saving annotation\n",
    "        et = etree.ElementTree(xml_root)\n",
    "        xml_path = os.path.join(SAVE_PATH, folder, \"annotations/\", string_id+\".xml\")\n",
    "        check_dirs(xml_path)\n",
    "        with open(xml_path, 'wb') as xml_file:\n",
    "            et.write(xml_file, pretty_print=True)\n",
    "        \n",
    "        # Incrementing id\n",
    "        lock.acquire()\n",
    "        wid = n_id.value\n",
    "        if verbose:\n",
    "            clear_output(wait=True)\n",
    "            print(\"Worker \", wnumber, \": \", (wid+1), \"/\", n_images)\n",
    "        n_id.value += 1\n",
    "        lock.release()\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "n_process = 4\n",
    "verbose = True\n",
    "save_maps()\n",
    "\n",
    "# Generating training set\n",
    "folder = \"train/\"\n",
    "n_images_train = (1-PERCENTAGE_VAL)*TOTAL_N_IMAGES\n",
    "n_id_train = multiprocessing.Value('i', 0)\n",
    "lock_train = multiprocessing.Lock()\n",
    "print(\"Training: Generating\", n_images_train, \"images with \", n_process, \" workers !\")\n",
    "for i in range(n_process):\n",
    "    p = multiprocessing.Process(target=worker, args=(lock_train, \n",
    "                                                     n_id_train, \n",
    "                                                     i,\n",
    "                                                     True,\n",
    "                                                     n_images_train, \n",
    "                                                     verbose, \n",
    "                                                     folder))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "# Wait for jobs to end\n",
    "for job in jobs:\n",
    "    job.join()\n",
    "    \n",
    "# Generating validation set\n",
    "folder = \"val/\"\n",
    "n_images_val = PERCENTAGE_VAL*TOTAL_N_IMAGES\n",
    "n_id_val = multiprocessing.Value('i', 0)\n",
    "lock_val = multiprocessing.Lock()\n",
    "print(\"Validation: Generating\", n_images_val, \"images with \", n_process, \" workers !\")\n",
    "for i in range(n_process):\n",
    "    p = multiprocessing.Process(target=worker, args=(lock_val, \n",
    "                                                     n_id_val, \n",
    "                                                     i,\n",
    "                                                     False,\n",
    "                                                     n_images_val, \n",
    "                                                     verbose, \n",
    "                                                     folder))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for job in jobs:\n",
    "    job.join()\n",
    "print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
