{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import cv2\n",
    "from sys import getsizeof\n",
    "from IPython.display import display\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import *\n",
    "#tf.config.optimizer.set_jit(True)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 20\n",
    "TRAIN_P = 0.8\n",
    "VAL_P = 0.2\n",
    "TEST_P = 0\n",
    "\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../utils/data_utils.ipynb\n",
    "\n",
    "id_map = get_selected_taxons(\"../../selected_taxons.txt\")\n",
    "n_classes = len(list(id_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files, labels = get_dataset()\n",
    "NB_SAMPLES = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image, label in full_dataset.take(1):\n",
    "#    print(image.numpy().shape)\n",
    "#    print(label.numpy())\n",
    "#    display(Image.fromarray(np.uint8(image.numpy()*255)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_load_image(image_path, label):\n",
    "    img_file = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img_file, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32) #0-1 range\n",
    "    return img, label\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices((files, tf.keras.utils.to_categorical(labels, num_classes=None, dtype='float32')))\n",
    "full_dataset = full_dataset.shuffle(len(files))\n",
    "full_dataset = full_dataset.map(cb_load_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "##full_dataset = full_dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = len(files)\n",
    "train_size = int(TRAIN_P * DATASET_SIZE)\n",
    "val_size = int(VAL_P * DATASET_SIZE)\n",
    "test_size = int(TEST_P * DATASET_SIZE)\n",
    "\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "tmp_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = tmp_dataset.take(val_size)\n",
    "tmp_dataset = full_dataset.skip(val_size)\n",
    "test_dataset = tmp_dataset.take(test_size)\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_dataset).numpy())\n",
    "print(tf.data.experimental.cardinality(val_dataset).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model desgin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching base model\n",
    "#base_model = Xception(include_top=False, weights='imagenet', input_shape=(256, 256, 3), pooling=None)\n",
    "input_tensor = Input(shape=(256, 256, 3))\n",
    "base_model = InceptionV3(weights='imagenet', input_tensor=input_tensor, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model for specifiv case\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "out = Dense(230, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting tensorboard\n",
    "!rm -rf LOG_DIR\n",
    "log_dir = LOG_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(int(0.1*N_EPOCHS), \"epochs composed of\", (int(train_size/BATCH_SIZE)-1), \"batches (steps) of\", BATCH_SIZE, \"images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_train1 = 3\n",
    "\n",
    "train_train1 = train_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "val_train1 = val_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_train1, \n",
    "          epochs=n_epochs_train1, \n",
    "          steps_per_epoch=int(train_size/BATCH_SIZE),\n",
    "          use_multiprocessing=True, \n",
    "          validation_data=val_train1,\n",
    "          validation_steps=int(val_size/BATCH_SIZE),\n",
    "          callbacks=[tensorboard_callback],\n",
    "          initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train1 = train_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "val_train1 = val_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "\n",
    "print(n_epochs_train1*int(train_size/BATCH_SIZE))\n",
    "print(tf.data.experimental.cardinality(train_train1).numpy())\n",
    "print(n_epochs_train1*int(val_size/BATCH_SIZE))\n",
    "print(int(tf.data.experimental.cardinality(val_train1).numpy()))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 2 last inceptions blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_train2 = n_epochs_train1+10\n",
    "\n",
    "train_train2 = train_dataset.repeat(n_epochs_train2).batch(BATCH_SIZE)\n",
    "val_train2 = val_dataset.repeat(n_epochs_train2).batch(BATCH_SIZE)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_train2, \n",
    "          epochs=n_epochs_train2, \n",
    "          steps_per_epoch=int(train_size/BATCH_SIZE)-1,\n",
    "          use_multiprocessing=True, \n",
    "          validation_data=val_train2,\n",
    "          validation_steps=int(val_size/BATCH_SIZE)-1,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          initial_epoch=n_epochs_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_train3 = n_epochs_train2+20\n",
    "\n",
    "train_train3 = train_dataset.repeat(n_epochs_train3).batch(BATCH_SIZE)\n",
    "val_train3 = val_dataset.repeat(n_epochs_train3).batch(BATCH_SIZE)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_train3, \n",
    "          epochs=n_epochs_train3, \n",
    "          steps_per_epoch=int(train_size/BATCH_SIZE)-1,\n",
    "          use_multiprocessing=True, \n",
    "          validation_data=val_train3,\n",
    "          validation_steps=int(val_size/BATCH_SIZE)-1,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          initial_epoch=n_epochs_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, SAVED_MODELS_ROOT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
