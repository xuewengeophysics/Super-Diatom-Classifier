{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython.display import display\n",
    "import math, random\n",
    "import time, datetime, sys, os, shutil\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "#tf.config.optimizer.set_jit(True)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../utils/data_utils.ipynb\n",
    "\n",
    "id_map = get_selected_taxons(SELECTED_TAXONS)\n",
    "id_map_inv = get_selected_taxons(SELECTED_TAXONS, inv=True)\n",
    "n_classes = len(list(id_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing panda arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'171/171'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 6144\n",
      "Test length: 4915 - n classes: 171\n",
      "Test length: 1229 - n classes: 167\n"
     ]
    }
   ],
   "source": [
    "X, y, _ = get_dataset(ids=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Original length:\", len(X))\n",
    "print(\"Test length:\", len(X_train), \"- n classes:\", len(np.unique(y_train)))\n",
    "print(\"Test length:\", len(X_test), \"- n classes:\", len(np.unique(y_test)))\n",
    "# Balance dataset\n",
    "train_dict = {}\n",
    "for file, label in zip(X_train, y_train):\n",
    "    train_dict.setdefault(label, []).append(file)\n",
    "max_samples = np.max([len(train_dict[taxon_id]) for taxon_id in train_dict])\n",
    "X_train = []\n",
    "y_train = []\n",
    "for taxon_id in train_dict:\n",
    "    ratio = np.ceil(max_samples/len(train_dict[taxon_id]))\n",
    "    tmp = np.repeat(train_dict[taxon_id], ratio)\n",
    "    np.random.shuffle(tmp)\n",
    "    train_dict[taxon_id] = tmp[0:max_samples]\n",
    "    X_train.extend(tmp[0:max_samples])\n",
    "    y_train.extend([taxon_id]*max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20691 1229\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>png_path</th>\n",
       "      <th>taxon_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>NERI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>GMIN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>NANT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>CMED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>CMLF</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            png_path taxon_id\n",
       "0  /mnt/nvme-storage/pfauregi/datasets/condensed/...     NERI\n",
       "1  /mnt/nvme-storage/pfauregi/datasets/condensed/...     GMIN\n",
       "2  /mnt/nvme-storage/pfauregi/datasets/condensed/...     NANT\n",
       "3  /mnt/nvme-storage/pfauregi/datasets/condensed/...     CMED\n",
       "4  /mnt/nvme-storage/pfauregi/datasets/condensed/...     CMLF"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = {'png_path':  X_train, 'taxon_id': y_train}\n",
    "data_test = {'png_path':  X_test, 'taxon_id': y_test}\n",
    "\n",
    "df_train = pd.DataFrame(data_train, columns = ['png_path', 'taxon_id'])\n",
    "df_test = pd.DataFrame(data_test, columns = ['png_path', 'taxon_id'])\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Prtining some infos\n",
    "print(len(df_train), len(df_test))\n",
    "df_train.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20691 validated image filenames belonging to 230 classes.\n",
      "Found 1229 validated image filenames belonging to 230 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                         rotation_range=90, \n",
    "                         brightness_range=[0.8,1.2], \n",
    "                         horizontal_flip=True, \n",
    "                         vertical_flip=True,\n",
    "                         fill_mode='nearest',\n",
    "                         width_shift_range=40,\n",
    "                         height_shift_range=40,\n",
    "                         zoom_range=0.2,\n",
    "                         validation_split=0.2,\n",
    "                         #featurewise_std_normalization=True,\n",
    "                         data_format=\"channels_last\")\n",
    "\n",
    "classes_array = get_id_array(id_map)\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        x_col='png_path',\n",
    "        y_col='taxon_id',\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        classes=classes_array,\n",
    "        class_mode='categorical')\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df_test,\n",
    "        x_col='png_path',\n",
    "        y_col='taxon_id',\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        classes=classes_array,\n",
    "        class_mode='categorical')\n",
    "\n",
    "train_spe = train_generator.samples // BATCH_SIZE\n",
    "val_spe = val_generator.samples // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "stop = False\n",
    "for batch in train_generator:\n",
    "    images = batch[0]\n",
    "    labels = batch[1]\n",
    "    for i in range(images.shape[0]):\n",
    "        print(np.argmax(labels[i]))\n",
    "        image = (images[i,:,:,:]*255).astype('uint8')\n",
    "        #print(image)\n",
    "        display(Image.fromarray(image))\n",
    "        i+=1\n",
    "        if i>=1:\n",
    "            stop = True\n",
    "            break\n",
    "    if stop: break;\n",
    "    #display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model desgin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching base model\n",
    "#base_model = Xception(include_top=False, weights='imagenet', input_shape=(256, 256, 3), pooling=None)\n",
    "input_tensor = Input(shape=(256, 256, 3))\n",
    "base_model = InceptionV3(weights='imagenet', input_tensor=input_tensor, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model for specifiv case\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "out = Dense(230, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_all_files_in_folder(LOG_DIR)\n",
    "delete_all_files_in_folder(SAVED_MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting tensorboard\n",
    "log_dir = LOG_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file=os.path.join(SAVED_MODELS_ROOT, \"model.log\")\n",
    "csv_logger = CSVLogger(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epochs composed of 646 batches (steps) of 32 images.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 646 steps, validate for 38 steps\n",
      "Epoch 1/3\n",
      "  1/646 [..............................] - ETA: 58:53 - loss: 5.5680 - accuracy: 0.0000e+00WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.184966). Check your callbacks.\n",
      "646/646 [==============================] - 276s 427ms/step - loss: 2.7110 - accuracy: 0.3761 - val_loss: 4.6534 - val_accuracy: 0.0995\n",
      "Epoch 2/3\n",
      "646/646 [==============================] - 267s 414ms/step - loss: 1.3030 - accuracy: 0.6328 - val_loss: 4.8508 - val_accuracy: 0.1431\n",
      "Epoch 3/3\n",
      "646/646 [==============================] - 268s 415ms/step - loss: 1.0339 - accuracy: 0.6904 - val_loss: 5.0072 - val_accuracy: 0.1530\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1b475eb00>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "last_epoch = get_last_epoch(log_file)\n",
    "\n",
    "print(n_epochs, \"epochs composed of\", train_spe, \"batches (steps) of\", BATCH_SIZE, \"images.\")\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_generator, \n",
    "          epochs=last_epoch+n_epochs, \n",
    "          steps_per_epoch=train_spe,\n",
    "          use_multiprocessing=False, \n",
    "          validation_data=val_generator,\n",
    "          validation_steps=val_spe,\n",
    "          callbacks=[tensorboard_callback, csv_logger],\n",
    "          initial_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 2 last inceptions blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 epochs composed of 646 batches (steps) of 32 images.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 646 steps, validate for 38 steps\n",
      "Epoch 3/9\n",
      "646/646 [==============================] - 272s 421ms/step - loss: 0.9006 - accuracy: 0.7223 - val_loss: 4.2861 - val_accuracy: 0.2706\n",
      "Epoch 4/9\n",
      "646/646 [==============================] - 268s 414ms/step - loss: 0.4783 - accuracy: 0.8431 - val_loss: 5.9722 - val_accuracy: 0.2311\n",
      "Epoch 5/9\n",
      "646/646 [==============================] - 267s 414ms/step - loss: 0.3722 - accuracy: 0.8778 - val_loss: 4.0993 - val_accuracy: 0.2878\n",
      "Epoch 6/9\n",
      "646/646 [==============================] - 268s 415ms/step - loss: 0.3143 - accuracy: 0.8935 - val_loss: 3.3611 - val_accuracy: 0.3322\n",
      "Epoch 7/9\n",
      "646/646 [==============================] - 268s 415ms/step - loss: 0.2674 - accuracy: 0.9116 - val_loss: 4.6790 - val_accuracy: 0.2895\n",
      "Epoch 8/9\n",
      "646/646 [==============================] - 268s 415ms/step - loss: 0.2477 - accuracy: 0.9194 - val_loss: 4.2780 - val_accuracy: 0.3281\n",
      "Epoch 9/9\n",
      "646/646 [==============================] - 268s 416ms/step - loss: 0.2251 - accuracy: 0.9265 - val_loss: 4.4055 - val_accuracy: 0.2911\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1a47e3cc0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 7\n",
    "last_epoch = get_last_epoch(log_file)\n",
    "\n",
    "print(n_epochs, \"epochs composed of\", train_spe, \"batches (steps) of\", BATCH_SIZE, \"images.\")\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(train_generator, \n",
    "          epochs=last_epoch+n_epochs, \n",
    "          steps_per_epoch=train_spe,\n",
    "          use_multiprocessing=False,\n",
    "          validation_data=val_generator,\n",
    "          validation_steps=val_spe,\n",
    "          callbacks=[tensorboard_callback, csv_logger],\n",
    "          initial_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 epochs composed of 646 batches (steps) of 32 images.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 646 steps, validate for 38 steps\n",
      "Epoch 9/28\n",
      "646/646 [==============================] - 273s 422ms/step - loss: 0.8852 - accuracy: 0.7378 - val_loss: 1.4018 - val_accuracy: 0.6472\n",
      "Epoch 10/28\n",
      "646/646 [==============================] - 265s 411ms/step - loss: 0.4395 - accuracy: 0.8520 - val_loss: 1.0385 - val_accuracy: 0.7574\n",
      "Epoch 11/28\n",
      "646/646 [==============================] - 265s 410ms/step - loss: 0.3543 - accuracy: 0.8825 - val_loss: 0.9637 - val_accuracy: 0.7229\n",
      "Epoch 12/28\n",
      "646/646 [==============================] - 267s 413ms/step - loss: 0.2891 - accuracy: 0.9029 - val_loss: 1.3534 - val_accuracy: 0.6826\n",
      "Epoch 13/28\n",
      "646/646 [==============================] - 266s 412ms/step - loss: 0.2692 - accuracy: 0.9112 - val_loss: 1.0368 - val_accuracy: 0.7590\n",
      "Epoch 14/28\n",
      "646/646 [==============================] - 266s 412ms/step - loss: 0.2342 - accuracy: 0.9234 - val_loss: 0.9616 - val_accuracy: 0.7615\n",
      "Epoch 15/28\n",
      "646/646 [==============================] - 266s 411ms/step - loss: 0.2148 - accuracy: 0.9277 - val_loss: 0.8583 - val_accuracy: 0.7887\n",
      "Epoch 16/28\n",
      "646/646 [==============================] - 265s 411ms/step - loss: 0.1968 - accuracy: 0.9348 - val_loss: 0.6553 - val_accuracy: 0.8257\n",
      "Epoch 17/28\n",
      "646/646 [==============================] - 266s 411ms/step - loss: 0.1833 - accuracy: 0.9406 - val_loss: 0.8488 - val_accuracy: 0.7829\n",
      "Epoch 18/28\n",
      "646/646 [==============================] - 266s 411ms/step - loss: 0.1639 - accuracy: 0.9457 - val_loss: 0.7301 - val_accuracy: 0.8355\n",
      "Epoch 19/28\n",
      "646/646 [==============================] - 265s 410ms/step - loss: 0.1512 - accuracy: 0.9493 - val_loss: 0.8893 - val_accuracy: 0.7936\n",
      "Epoch 20/28\n",
      "646/646 [==============================] - 266s 412ms/step - loss: 0.1518 - accuracy: 0.9494 - val_loss: 0.8319 - val_accuracy: 0.8117\n",
      "Epoch 21/28\n",
      "646/646 [==============================] - 265s 411ms/step - loss: 0.1312 - accuracy: 0.9561 - val_loss: 0.8413 - val_accuracy: 0.8026\n",
      "Epoch 22/28\n",
      "646/646 [==============================] - 266s 412ms/step - loss: 0.1091 - accuracy: 0.9648 - val_loss: 1.3752 - val_accuracy: 0.7360\n",
      "Epoch 23/28\n",
      "646/646 [==============================] - 266s 411ms/step - loss: 0.1293 - accuracy: 0.9572 - val_loss: 0.8668 - val_accuracy: 0.8067\n",
      "Epoch 24/28\n",
      "646/646 [==============================] - 266s 411ms/step - loss: 0.1214 - accuracy: 0.9613 - val_loss: 0.8194 - val_accuracy: 0.8224\n",
      "Epoch 25/28\n",
      "646/646 [==============================] - 266s 411ms/step - loss: 0.0930 - accuracy: 0.9683 - val_loss: 0.5997 - val_accuracy: 0.8808\n",
      "Epoch 26/28\n",
      "646/646 [==============================] - 265s 410ms/step - loss: 0.1137 - accuracy: 0.9630 - val_loss: 0.8168 - val_accuracy: 0.8322\n",
      "Epoch 27/28\n",
      "646/646 [==============================] - 266s 412ms/step - loss: 0.0912 - accuracy: 0.9703 - val_loss: 0.7806 - val_accuracy: 0.8248\n",
      "Epoch 28/28\n",
      "646/646 [==============================] - 265s 410ms/step - loss: 0.0964 - accuracy: 0.9682 - val_loss: 0.6947 - val_accuracy: 0.8405\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fd1b475ebe0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "last_epoch = get_last_epoch(log_file)\n",
    "\n",
    "print(n_epochs, \"epochs composed of\", train_spe, \"batches (steps) of\", BATCH_SIZE, \"images.\")\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(train_generator, \n",
    "          epochs=last_epoch+n_epochs, \n",
    "          steps_per_epoch=train_spe,\n",
    "          use_multiprocessing=False, \n",
    "          validation_data=val_generator,\n",
    "          validation_steps=val_spe,\n",
    "          callbacks=[tensorboard_callback, csv_logger],\n",
    "          initial_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ./saved_models/model.json\n",
      "Saved weights to ./saved_models/model.h5\n"
     ]
    }
   ],
   "source": [
    "save_model(model, SAVED_MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
