{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "WARNING:tensorflow:From <ipython-input-1-4db411560540>:41: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython.display import display\n",
    "import math, random\n",
    "import time, datetime, sys, os, shutil\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import CSVLogger\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "#tf.config.optimizer.set_jit(True)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../utils/data_utils.ipynb\n",
    "\n",
    "id_map = get_selected_taxons(SELECTED_TAXONS)\n",
    "id_map_inv = get_selected_taxons(SELECTED_TAXONS, inv=True)\n",
    "n_classes = len(list(id_map.keys()))\n",
    "\n",
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing panda arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'171/171'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 6144\n",
      "Test length: 4915 - n classes: 171\n",
      "Test length: 1229 - n classes: 167\n"
     ]
    }
   ],
   "source": [
    "X, y, _ = get_dataset(DATASET_PATH, ids=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"Original length:\", len(X))\n",
    "print(\"Test length:\", len(X_train), \"- n classes:\", len(np.unique(y_train)))\n",
    "print(\"Test length:\", len(X_test), \"- n classes:\", len(np.unique(y_test)))\n",
    "# Balance dataset\n",
    "train_dict = {}\n",
    "for file, label in zip(X_train, y_train):\n",
    "    train_dict.setdefault(label, []).append(file)\n",
    "max_samples = np.max([len(train_dict[taxon_id]) for taxon_id in train_dict])\n",
    "X_train = []\n",
    "y_train = []\n",
    "for taxon_id in train_dict:\n",
    "    ratio = np.ceil(max_samples/len(train_dict[taxon_id]))\n",
    "    tmp = np.repeat(train_dict[taxon_id], ratio)\n",
    "    np.random.shuffle(tmp)\n",
    "    train_dict[taxon_id] = tmp[0:max_samples]\n",
    "    X_train.extend(tmp[0:max_samples])\n",
    "    y_train.extend([taxon_id]*max_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20691 1229\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>png_path</th>\n",
       "      <th>taxon_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>NVEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>CMLF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>AMID</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>PSBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/mnt/nvme-storage/pfauregi/datasets/condensed/...</td>\n",
       "      <td>CMEN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            png_path taxon_id\n",
       "0  /mnt/nvme-storage/pfauregi/datasets/condensed/...     NVEN\n",
       "1  /mnt/nvme-storage/pfauregi/datasets/condensed/...     CMLF\n",
       "2  /mnt/nvme-storage/pfauregi/datasets/condensed/...     AMID\n",
       "3  /mnt/nvme-storage/pfauregi/datasets/condensed/...     PSBR\n",
       "4  /mnt/nvme-storage/pfauregi/datasets/condensed/...     CMEN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = {'png_path':  X_train, 'taxon_id': y_train}\n",
    "data_test = {'png_path':  X_test, 'taxon_id': y_test}\n",
    "\n",
    "df_train = pd.DataFrame(data_train, columns = ['png_path', 'taxon_id'])\n",
    "df_test = pd.DataFrame(data_test, columns = ['png_path', 'taxon_id'])\n",
    "df_train = df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Prtining some infos\n",
    "print(len(df_train), len(df_test))\n",
    "df_train.head()\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20691 validated image filenames belonging to 230 classes.\n",
      "Found 1229 validated image filenames belonging to 230 classes.\n"
     ]
    }
   ],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                         rotation_range=90, \n",
    "                         brightness_range=[0.8,1.2], \n",
    "                         horizontal_flip=True, \n",
    "                         vertical_flip=True,\n",
    "                         fill_mode='nearest',\n",
    "                         width_shift_range=40,\n",
    "                         height_shift_range=40,\n",
    "                         zoom_range=0.2,\n",
    "                         validation_split=0.2,\n",
    "                         #featurewise_std_normalization=True,\n",
    "                         data_format=\"channels_last\")\n",
    "\n",
    "classes_array = id_map\n",
    "train_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df_train,\n",
    "        x_col='png_path',\n",
    "        y_col='taxon_id',\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        classes=classes_array,\n",
    "        class_mode='categorical')\n",
    "val_generator = datagen.flow_from_dataframe(\n",
    "        dataframe=df_test,\n",
    "        x_col='png_path',\n",
    "        y_col='taxon_id',\n",
    "        target_size=(256, 256),\n",
    "        batch_size=32,\n",
    "        classes=classes_array,\n",
    "        class_mode='categorical')\n",
    "\n",
    "train_spe = train_generator.samples // BATCH_SIZE\n",
    "val_spe = val_generator.samples // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_indices = train_generator.class_indices\n",
    "f = open(os.path.join(SAVED_MODELS_ROOT, 'model_id_map.csv'), 'w')\n",
    "with f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"taxon\", \"id\"])\n",
    "    for taxon in class_indices:\n",
    "        writer.writerow([taxon, class_indices[taxon]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "stop = False\n",
    "for batch in train_generator:\n",
    "    images = batch[0]\n",
    "    labels = batch[1]\n",
    "    for i in range(images.shape[0]):\n",
    "        print(np.argmax(labels[i]))\n",
    "        image = (images[i,:,:,:]*255).astype('uint8')\n",
    "        #print(image)\n",
    "        display(Image.fromarray(image))\n",
    "        i+=1\n",
    "        if i>=1:\n",
    "            stop = True\n",
    "            break\n",
    "    if stop: break;\n",
    "    #display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model desgin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching base model\n",
    "#base_model = Xception(include_top=False, weights='imagenet', input_shape=(256, 256, 3), pooling=None)\n",
    "input_tensor = Input(shape=(256, 256, 3))\n",
    "base_model = InceptionV3(weights='imagenet', input_tensor=input_tensor, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model for specifiv case\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "out = Dense(230, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_all_files_in_folder(LOG_DIR)\n",
    "delete_all_files_in_folder(SAVED_MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting tensorboard\n",
    "log_dir = LOG_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file=os.path.join(SAVED_MODELS_ROOT, \"model.log\")\n",
    "csv_logger = CSVLogger(log_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 epochs composed of 646 batches (steps) of 32 images.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 646 steps, validate for 38 steps\n",
      "Epoch 1/3\n",
      "  1/646 [..............................] - ETA: 49:02 - loss: 5.3766 - accuracy: 0.0312WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.156832). Check your callbacks.\n",
      "646/646 [==============================] - 263s 407ms/step - loss: 2.8096 - accuracy: 0.3524 - val_loss: 4.6205 - val_accuracy: 0.1143\n",
      "Epoch 2/3\n",
      "646/646 [==============================] - 259s 401ms/step - loss: 1.4531 - accuracy: 0.5904 - val_loss: 4.7965 - val_accuracy: 0.1645\n",
      "Epoch 3/3\n",
      "646/646 [==============================] - 260s 402ms/step - loss: 1.1832 - accuracy: 0.6533 - val_loss: 5.2707 - val_accuracy: 0.1324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa340410048>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "last_epoch = get_last_epoch(log_file)\n",
    "\n",
    "print(n_epochs, \"epochs composed of\", train_spe, \"batches (steps) of\", BATCH_SIZE, \"images.\")\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_generator, \n",
    "          epochs=last_epoch+n_epochs, \n",
    "          steps_per_epoch=train_spe,\n",
    "          use_multiprocessing=False, \n",
    "          validation_data=val_generator,\n",
    "          validation_steps=val_spe,\n",
    "          callbacks=[tensorboard_callback, csv_logger],\n",
    "          initial_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 2 last inceptions blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 epochs composed of 646 batches (steps) of 32 images.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 646 steps, validate for 38 steps\n",
      "Epoch 3/9\n",
      "646/646 [==============================] - 264s 408ms/step - loss: 0.9986 - accuracy: 0.6970 - val_loss: 5.1816 - val_accuracy: 0.2336\n",
      "Epoch 4/9\n",
      "646/646 [==============================] - 260s 403ms/step - loss: 0.5358 - accuracy: 0.8258 - val_loss: 4.1440 - val_accuracy: 0.2763\n",
      "Epoch 5/9\n",
      "646/646 [==============================] - 262s 406ms/step - loss: 0.4280 - accuracy: 0.8602 - val_loss: 4.4547 - val_accuracy: 0.3207\n",
      "Epoch 6/9\n",
      "646/646 [==============================] - 260s 403ms/step - loss: 0.3535 - accuracy: 0.8815 - val_loss: 3.6655 - val_accuracy: 0.3586\n",
      "Epoch 7/9\n",
      "646/646 [==============================] - 260s 402ms/step - loss: 0.3129 - accuracy: 0.8938 - val_loss: 4.5725 - val_accuracy: 0.2911\n",
      "Epoch 8/9\n",
      "646/646 [==============================] - 261s 403ms/step - loss: 0.2832 - accuracy: 0.9057 - val_loss: 4.6052 - val_accuracy: 0.2878\n",
      "Epoch 9/9\n",
      "646/646 [==============================] - 260s 402ms/step - loss: 0.2511 - accuracy: 0.9172 - val_loss: 5.1830 - val_accuracy: 0.3035\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa3315c66d8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 7\n",
    "last_epoch = get_last_epoch(log_file)\n",
    "\n",
    "print(n_epochs, \"epochs composed of\", train_spe, \"batches (steps) of\", BATCH_SIZE, \"images.\")\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(train_generator, \n",
    "          epochs=last_epoch+n_epochs, \n",
    "          steps_per_epoch=train_spe,\n",
    "          use_multiprocessing=False,\n",
    "          validation_data=val_generator,\n",
    "          validation_steps=val_spe,\n",
    "          callbacks=[tensorboard_callback, csv_logger],\n",
    "          initial_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training full model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 epochs composed of 646 batches (steps) of 32 images.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 646 steps, validate for 38 steps\n",
      "Epoch 9/32\n",
      "646/646 [==============================] - 265s 410ms/step - loss: 0.9534 - accuracy: 0.7148 - val_loss: 1.2955 - val_accuracy: 0.6456\n",
      "Epoch 10/32\n",
      "646/646 [==============================] - 260s 402ms/step - loss: 0.4702 - accuracy: 0.8437 - val_loss: 1.9315 - val_accuracy: 0.5946\n",
      "Epoch 11/32\n",
      "646/646 [==============================] - 261s 403ms/step - loss: 0.3496 - accuracy: 0.8834 - val_loss: 1.7296 - val_accuracy: 0.5929\n",
      "Epoch 12/32\n",
      "646/646 [==============================] - 264s 409ms/step - loss: 0.2975 - accuracy: 0.9027 - val_loss: 1.4118 - val_accuracy: 0.6735\n",
      "Epoch 13/32\n",
      "646/646 [==============================] - 260s 402ms/step - loss: 0.2660 - accuracy: 0.9103 - val_loss: 1.0759 - val_accuracy: 0.7220\n",
      "Epoch 14/32\n",
      "646/646 [==============================] - 259s 401ms/step - loss: 0.2420 - accuracy: 0.9183 - val_loss: 1.0820 - val_accuracy: 0.7368\n",
      "Epoch 15/32\n",
      "646/646 [==============================] - 259s 402ms/step - loss: 0.2201 - accuracy: 0.9280 - val_loss: 1.4371 - val_accuracy: 0.6809\n",
      "Epoch 16/32\n",
      "646/646 [==============================] - 260s 402ms/step - loss: 0.1908 - accuracy: 0.9365 - val_loss: 0.9485 - val_accuracy: 0.7788\n",
      "Epoch 17/32\n",
      "646/646 [==============================] - 260s 402ms/step - loss: 0.1877 - accuracy: 0.9386 - val_loss: 0.7181 - val_accuracy: 0.8257\n",
      "Epoch 18/32\n",
      "646/646 [==============================] - 259s 402ms/step - loss: 0.1621 - accuracy: 0.9452 - val_loss: 1.0685 - val_accuracy: 0.7747\n",
      "Epoch 19/32\n",
      "646/646 [==============================] - 260s 403ms/step - loss: 0.1549 - accuracy: 0.9486 - val_loss: 1.7808 - val_accuracy: 0.6521\n",
      "Epoch 20/32\n",
      "646/646 [==============================] - 259s 401ms/step - loss: 0.1520 - accuracy: 0.9499 - val_loss: 1.2672 - val_accuracy: 0.7196\n",
      "Epoch 21/32\n",
      "646/646 [==============================] - 260s 402ms/step - loss: 0.1391 - accuracy: 0.9543 - val_loss: 0.8763 - val_accuracy: 0.7903\n",
      "Epoch 22/32\n",
      "646/646 [==============================] - 259s 402ms/step - loss: 0.1278 - accuracy: 0.9590 - val_loss: 1.4376 - val_accuracy: 0.7229\n",
      "Epoch 23/32\n",
      "646/646 [==============================] - 259s 401ms/step - loss: 0.1199 - accuracy: 0.9606 - val_loss: 0.9345 - val_accuracy: 0.7862\n",
      "Epoch 24/32\n",
      "646/646 [==============================] - 259s 402ms/step - loss: 0.1244 - accuracy: 0.9598 - val_loss: 0.9988 - val_accuracy: 0.7664\n",
      "Epoch 25/32\n",
      "646/646 [==============================] - 261s 404ms/step - loss: 0.1177 - accuracy: 0.9624 - val_loss: 1.0342 - val_accuracy: 0.8059\n",
      "Epoch 26/32\n",
      "646/646 [==============================] - 261s 404ms/step - loss: 0.1101 - accuracy: 0.9641 - val_loss: 1.0917 - val_accuracy: 0.7656\n",
      "Epoch 27/32\n",
      "646/646 [==============================] - 260s 403ms/step - loss: 0.0962 - accuracy: 0.9681 - val_loss: 0.8535 - val_accuracy: 0.8363\n",
      "Epoch 28/32\n",
      "646/646 [==============================] - 261s 404ms/step - loss: 0.0859 - accuracy: 0.9735 - val_loss: 0.6552 - val_accuracy: 0.8725\n",
      "Epoch 29/32\n",
      "646/646 [==============================] - 261s 405ms/step - loss: 0.0876 - accuracy: 0.9728 - val_loss: 0.8802 - val_accuracy: 0.8191\n",
      "Epoch 30/32\n",
      "646/646 [==============================] - 269s 416ms/step - loss: 0.0869 - accuracy: 0.9716 - val_loss: 1.0243 - val_accuracy: 0.8051\n",
      "Epoch 31/32\n",
      "646/646 [==============================] - 261s 404ms/step - loss: 0.0918 - accuracy: 0.9695 - val_loss: 0.7885 - val_accuracy: 0.8421\n",
      "Epoch 32/32\n",
      "646/646 [==============================] - 261s 403ms/step - loss: 0.0717 - accuracy: 0.9769 - val_loss: 0.7300 - val_accuracy: 0.8602\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa3402966a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 24\n",
    "last_epoch = get_last_epoch(log_file)\n",
    "\n",
    "print(n_epochs, \"epochs composed of\", train_spe, \"batches (steps) of\", BATCH_SIZE, \"images.\")\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(train_generator, \n",
    "          epochs=last_epoch+n_epochs, \n",
    "          steps_per_epoch=train_spe,\n",
    "          use_multiprocessing=False, \n",
    "          validation_data=val_generator,\n",
    "          validation_steps=val_spe,\n",
    "          callbacks=[tensorboard_callback, csv_logger],\n",
    "          initial_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to ./saved_models/model.json\n",
      "Saved weights to ./saved_models/model.h5\n"
     ]
    }
   ],
   "source": [
    "save_model(model, SAVED_MODELS_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
