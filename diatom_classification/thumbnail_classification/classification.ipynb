{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import cv2\n",
    "from sys import getsizeof\n",
    "from IPython.display import display\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import *\n",
    "#tf.config.optimizer.set_jit(True)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 20\n",
    "TRAIN_P = 0.70\n",
    "TEST_P = 0.15\n",
    "VAL_P = 0.15\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "\n",
    "id_map = get_selected_taxons()\n",
    "n_classes = len(list(id_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'185/185'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files, labels = get_dataset()\n",
    "NB_SAMPLES = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_load_image(image_path, label):\n",
    "    img_file = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img_file, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32) #0-1 range\n",
    "    return img, label\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices((files, tf.keras.utils.to_categorical(labels, num_classes=None, dtype='float32')))\n",
    "full_dataset = full_dataset.shuffle(len(files))\n",
    "full_dataset = full_dataset.map(cb_load_image, num_parallel_calls=AUTOTUNE)\n",
    "#full_dataset = full_dataset.batch(BATCH_SIZE)\n",
    "#full_dataset = full_dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image, label in full_dataset.take(1):\n",
    "#    print(image.numpy().shape)\n",
    "#    print(label.numpy())\n",
    "#    display(Image.fromarray(np.uint8(image.numpy()*255)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_SIZE = len(files)\n",
    "train_size = int(TRAIN_P * DATASET_SIZE)\n",
    "val_size = int(VAL_P * DATASET_SIZE)\n",
    "test_size = int(TEST_P * DATASET_SIZE)\n",
    "\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "tmp_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = tmp_dataset.skip(val_size)\n",
    "test_dataset = tmp_dataset.take(test_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model desgin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching base model\n",
    "#base_model = Xception(include_top=False, weights='imagenet', input_shape=(256, 256, 3), pooling=None)\n",
    "input_tensor = Input(shape=(256, 256, 3))\n",
    "base_model = InceptionV3(weights='imagenet', input_tensor=input_tensor, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model for specifiv case\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "out = Dense(230, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting tensorboard\n",
    "!rm -rf LOG_DIR\n",
    "log_dir = LOG_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epochs composed of 807 batches (steps) of 32 images.\n"
     ]
    }
   ],
   "source": [
    "print(int(0.1*N_EPOCHS), \"epochs composed of\", (int(train_size/BATCH_SIZE)-1), \"batches (steps) of\", BATCH_SIZE, \"images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 807 steps, validate for 172 steps\n",
      "Epoch 1/2\n",
      "  1/807 [..............................] - ETA: 1:02:38 - loss: 5.4849 - accuracy: 0.0000e+00WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.182651). Check your callbacks.\n",
      "807/807 [==============================] - 67s 83ms/step - loss: 3.0750 - accuracy: 0.2977 - val_loss: 5.2055 - val_accuracy: 0.0881\n",
      "Epoch 2/2\n",
      "807/807 [==============================] - 60s 75ms/step - loss: 1.5728 - accuracy: 0.5677 - val_loss: 5.0607 - val_accuracy: 0.1214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa72e23c518>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs_train1 = int(0.1*N_EPOCHS)\n",
    "\n",
    "train_train1 = train_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "val_train1 = val_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])\n",
    "model.fit(train_train1, \n",
    "          epochs=n_epochs_train1, \n",
    "          steps_per_epoch=int(train_size/BATCH_SIZE)-1,\n",
    "          use_multiprocessing=True, \n",
    "          validation_data=val_train1,\n",
    "          validation_steps=int(val_size/BATCH_SIZE)-1,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 807 steps, validate for 172 steps\n",
      "Epoch 1/10\n",
      "807/807 [==============================] - 137s 169ms/step - loss: 0.2283 - accuracy: 0.9353 - val_loss: 0.1982 - val_accuracy: 0.9446\n",
      "Epoch 2/10\n",
      "807/807 [==============================] - 131s 163ms/step - loss: 0.1699 - accuracy: 0.9544 - val_loss: 0.1793 - val_accuracy: 0.9491\n",
      "Epoch 3/10\n",
      "807/807 [==============================] - 133s 165ms/step - loss: 0.1383 - accuracy: 0.9655 - val_loss: 0.3079 - val_accuracy: 0.9052\n",
      "Epoch 4/10\n",
      "807/807 [==============================] - 133s 165ms/step - loss: 0.1182 - accuracy: 0.9717 - val_loss: 0.1401 - val_accuracy: 0.9626\n",
      "Epoch 5/10\n",
      "807/807 [==============================] - 131s 162ms/step - loss: 0.1047 - accuracy: 0.9757 - val_loss: 0.1109 - val_accuracy: 0.9746\n",
      "Epoch 6/10\n",
      "807/807 [==============================] - 131s 162ms/step - loss: 0.0889 - accuracy: 0.9814 - val_loss: 0.0766 - val_accuracy: 0.9831\n",
      "Epoch 7/10\n",
      "807/807 [==============================] - 131s 162ms/step - loss: 0.0821 - accuracy: 0.9830 - val_loss: 0.0522 - val_accuracy: 0.9906\n",
      "Epoch 8/10\n",
      "807/807 [==============================] - 131s 162ms/step - loss: 0.0738 - accuracy: 0.9861 - val_loss: 0.0986 - val_accuracy: 0.9762\n",
      "Epoch 9/10\n",
      "807/807 [==============================] - 131s 162ms/step - loss: 0.0654 - accuracy: 0.9882 - val_loss: 0.0520 - val_accuracy: 0.9906\n",
      "Epoch 10/10\n",
      "807/807 [==============================] - 131s 162ms/step - loss: 0.0600 - accuracy: 0.9900 - val_loss: 0.0714 - val_accuracy: 0.9846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa72cab0358>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs_train2 = int(0.5*N_EPOCHS)\n",
    "\n",
    "train_train2 = train_dataset.repeat(n_epochs_train2).batch(BATCH_SIZE)\n",
    "val_train2 = val_dataset.repeat(n_epochs_train2).batch(BATCH_SIZE)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = True\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "model.fit(train_train2, \n",
    "          epochs=n_epochs_train2, \n",
    "          steps_per_epoch=int(train_size/BATCH_SIZE)-1,\n",
    "          use_multiprocessing=True, \n",
    "          validation_data=val_train2,\n",
    "          validation_steps=int(val_size/BATCH_SIZE)-1,\n",
    "          callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model.save_weights(\"./saved_models/model.h5\")\n",
    "model_json = model.to_json()\n",
    "with open(\"./saved_models/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_dataset, use_multiprocessing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [np.argmax(pred) for pred in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int(0.1*N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
