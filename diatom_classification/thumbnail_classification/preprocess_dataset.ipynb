{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os, shutil\n",
    "from skimage.exposure import match_histograms\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../utils/data_utils.ipynb\n",
    "%run ../utils/image_utils.ipynb\n",
    "\n",
    "inf = float(\"inf\")\n",
    "RANGE = [50, 50] #[select, trim]\n",
    "TEST_SIZE = 0.1\n",
    "norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/nvme-storage/pfauregi/datasets/atlas/BRG\n",
      "/mnt/nvme-storage/pfauregi/datasets/atlas/IDF\n",
      "/mnt/nvme-storage/pfauregi/datasets/atlas/RA\n",
      "4200 images detected belonging to 84 classes found in 3 atlas!\n",
      "Eliminated taxon (unsufficient number of images): dict_keys(['PSXO', 'DCOF', 'CLNT', 'NULA', 'PHEL', 'GYAT', 'GYKU', 'AAMB', 'FMES', 'CMLF', 'MVAR', 'EMIN', 'FFVI', 'HUCO', 'MING', 'PTPU', 'AUGA', 'HARC', 'GSCI', 'PBIO', 'SPRG', 'SKPO', 'EEXI', 'KALA', 'AUAJ', 'TGES', 'CAMB', 'CTWE', 'SNIG', 'FCAD', 'CSNU', 'PLEV', 'EICD', 'THLA', 'SSGE', 'PDAO', 'LHLU', 'CPED', 'SBND', 'AUSU', 'PSAT', 'PROH', 'ADRI', 'UULN', 'CTPU', 'MPMI', 'SEAT', 'SLAC', 'NMIC', 'NGER', 'NTPT', 'NTRV', 'SPIN', 'AOVA', 'PGRN', 'NHEU', 'GYAC', 'NLAN', 'BPAX', 'SBKU', 'NACI', 'NRAD', 'GTRU', 'PULA', 'NFIL', 'DTEN', 'NREC', 'TLEV', 'ADRU', 'NLIN', 'NIPU', 'GBOB', 'SHTE', 'NPAL', 'NROS', 'FMOC', 'NSUA', 'NCPL', 'NVIR', 'NIPF', 'NFON', 'ENCM', 'NPAD', 'NRCS', 'NAMP', 'TAPI', 'NDIS', 'NSOC', 'ESUM', 'ADLA', 'ESLE', 'NAAN', 'GRHB', 'MAAT', 'NRHY', 'STMI', 'PSCA', 'ACLI', 'NEXI', 'ADAM', 'GLAT', 'CCMP', 'CNLP', 'MCCO', 'NVDA', 'ADSU', 'NESC', 'HLMO', 'PAPR', 'NNOT', 'CSLP', 'HVEN', 'GCLF', 'NSIA', 'NCIN'])\n",
      "Train dataset composed of 3780 images and 84 classes.\n",
      "Train dataset composed of 420 images and 84 classes.\n",
      "1 / 2\n",
      "2 / 2\n",
      "Finished !\n"
     ]
    }
   ],
   "source": [
    "# Loading reference image for histogram matching and saving ref img\n",
    "ref = cv2.imread(\"/mnt/nvme-storage/pfauregi/datasets/atlas/ref_img.png\", cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imwrite(os.path.join(SAVED_MODELS_ROOT, \"ref_img.png\"), ref)\n",
    "\n",
    "# Fetching files\n",
    "taxons_dict = {}\n",
    "selected_taxons = get_selected_taxons(SELECTED_TAXONS)\n",
    "for path in ATLAS_PATH:\n",
    "    print(path)\n",
    "    for taxon in os.listdir(path):\n",
    "        if taxon in selected_taxons.keys():\n",
    "        #if taxon in [\"AUGA\"]:\n",
    "            dir_path = os.path.join(path, taxon)\n",
    "            files = [f for f in os.listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "            for file in files:\n",
    "                split = file.split(\".\")\n",
    "                if (len(split)>1 and split[1]==\"png\"):\n",
    "                    source_file = os.path.join(dir_path, file)\n",
    "                    target_file = os.path.join(taxon, file)\n",
    "                    img_path = os.path.join(dir_path, file)\n",
    "                    taxons_dict.setdefault(taxon, []).append({\"source\": source_file, \"target\": target_file})\n",
    "\n",
    "# Filtering\n",
    "X, y = [], []\n",
    "eliminated_taxons = {}\n",
    "for taxon in taxons_dict:\n",
    "    files_tmp = taxons_dict[taxon]\n",
    "    if len(files_tmp)>=RANGE[0]:\n",
    "        if len(files_tmp)>=RANGE[1]: files_tmp = np.random.permutation(files_tmp)[0:RANGE[1]]\n",
    "        X.extend(files_tmp)\n",
    "        y.extend([taxon]*len(files_tmp))\n",
    "    else:\n",
    "        eliminated_taxons.setdefault(taxon, None)\n",
    "        \n",
    "print(len(X) ,\"images detected belonging to\", len(np.unique(y)), \"classes found in\",len(ATLAS_PATH),\"atlas!\")\n",
    "print(\"Eliminated taxon (unsufficient number of images):\", eliminated_taxons.keys())\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42, stratify=y)\n",
    "taxons_dict_train = {}\n",
    "taxons_dict_test = {}\n",
    "\n",
    "print(\"Train dataset composed of\", len(X_train), \"images and\", len(np.unique(y_train)), \"classes.\")\n",
    "print(\"Train dataset composed of\", len(X_test), \"images and\", len(np.unique(y_test)), \"classes.\")\n",
    "\n",
    "# Building dataset\n",
    "check_dirs(DATASET_PATH)\n",
    "delete_all_files_in_folder(DATASET_PATH)\n",
    "save_path = [TRAIN_DATASET_PATH, TEST_DATASET_PATH]\n",
    "Xs = [X_train, X_test]\n",
    "Ys = [y_train, y_test]\n",
    "for k in range(len(save_path)):\n",
    "    print((k+1),\"/\",len(save_path))\n",
    "    path = save_path[k]\n",
    "    X = Xs[k]\n",
    "    y = Ys[k]\n",
    "    for i in range(len(X)):\n",
    "        taxon = y[i]\n",
    "        source_file = X[i][\"source\"]\n",
    "        target_file = os.path.join(path, X[i][\"target\"])\n",
    "        check_dirs(target_file)\n",
    "        img = cv2.imread(source_file, cv2.IMREAD_GRAYSCALE)\n",
    "        if norm: img = match_histograms(img, ref, multichannel=False).astype(\"uint8\")\n",
    "        img = convert_to_square(img, new_size=256)\n",
    "        cv2.imwrite(target_file, img)\n",
    "print(\"Finished !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"taxon\", \"total\"]\n",
    "for atlas in ATLAS_PATH:\n",
    "    labels.append(atlas.split(\"/\")[-1])\n",
    "dict_array = []\n",
    "for taxon in sorted (taxons_dict.keys()):\n",
    "    total = len(taxons_dict[taxon])\n",
    "    row_dict = {\n",
    "        \"taxon\": taxon,\n",
    "        \"total\": total\n",
    "    }\n",
    "    for path in taxons_dict[taxon]:\n",
    "        aname = path[\"source\"].split(\"/\")[-3]\n",
    "        row_dict.setdefault(aname, 0)\n",
    "        row_dict[aname]+=1  \n",
    "    dict_array.append(row_dict)\n",
    "    \n",
    "f = open('./test.csv', 'w')\n",
    "with f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(labels)\n",
    "    for row_dict in dict_array:\n",
    "        row = []\n",
    "        for x in labels:\n",
    "            if x in row_dict:\n",
    "                row.append(row_dict[x])\n",
    "            else:\n",
    "                row.append(0)\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
