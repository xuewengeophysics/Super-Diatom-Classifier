{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os, shutil\n",
    "from skimage.exposure import match_histograms\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../utils/data_utils.ipynb\n",
    "%run ../utils/image_utils.ipynb\n",
    "\n",
    "inf = float(\"inf\")\n",
    "RANGE = [0, inf] #[select, trim]\n",
    "TEST_SIZE = 0.1\n",
    "norm = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /mnt/nvme-storage/pfauregi/datasets/Aqualitas/\n"
     ]
    }
   ],
   "source": [
    "# Get taxa list for filtering\n",
    "selected_taxa = get_taxa_list(FILTER_PATH)\n",
    "print(\"Filter:\", len(selected_taxa), \"taxa to select!\")\n",
    "\n",
    "# Loading reference image for histogram matching and saving ref img\n",
    "ref = cv2.imread(\"/mnt/nvme-storage/pfauregi/datasets/atlas/ref_img.png\", cv2.IMREAD_GRAYSCALE)\n",
    "cv2.imwrite(os.path.join(SAVED_MODELS_ROOT, \"ref_img.png\"), ref)\n",
    "\n",
    "# Fetching files\n",
    "taxa_dict = {}\n",
    "#selected_taxa = get_selected_taxa(SELECTED_TAXA)\n",
    "for path in DATASET:\n",
    "    print(\"Processing:\",path)\n",
    "    for taxon in os.listdir(path):\n",
    "        if taxon in selected_taxa:\n",
    "            dir_path = os.path.join(path, taxon)\n",
    "            files = [f for f in os.listdir(dir_path) if isfile(join(dir_path, f))]\n",
    "            for file in files:\n",
    "                split = file.split(\".\")\n",
    "                if (len(split)>1 and split[1] in [\"png\", \"tiff\", \"tif\"]):\n",
    "                    file_root = file.split(\".\")[0]\n",
    "                    source_file = os.path.join(dir_path, file)\n",
    "                    target_file = os.path.join(taxon, file_root+\".png\")\n",
    "                    taxa_dict.setdefault(taxon, []).append({\"source\": source_file, \"target\": target_file})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8668 images detected belonging to 80 classes found in 1 folder!\n",
      "Eliminated taxon (unsufficient number of images): []\n",
      "Train dataset composed of 7801 images and 80 classes.\n",
      "Test dataset composed of 867 images and 80 classes.\n",
      "1 / 2\n",
      "2 / 2\n",
      "Finished !\n"
     ]
    }
   ],
   "source": [
    "# Filtering\n",
    "X, y = [], []\n",
    "eliminated_taxa = {}\n",
    "for taxon in taxa_dict:\n",
    "    files_tmp = taxa_dict[taxon]\n",
    "    if len(files_tmp)>=RANGE[0]:\n",
    "        if len(files_tmp)>=RANGE[1]: files_tmp = np.random.permutation(files_tmp)[0:RANGE[1]]\n",
    "        X.extend(files_tmp)\n",
    "        y.extend([taxon]*len(files_tmp))\n",
    "    else:\n",
    "        eliminated_taxa.setdefault(taxon, None)\n",
    "eliminated_taxa = list(eliminated_taxa.keys())\n",
    "print(len(X) ,\"images detected belonging to\", len(np.unique(y)), \"classes found in\",len(DATASET),\"folder!\")\n",
    "print(\"Eliminated taxon (unsufficient number of images):\", eliminated_taxa)\n",
    "\n",
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=42, stratify=y)\n",
    "taxa_dict_train = {}\n",
    "taxa_dict_test = {}\n",
    "\n",
    "print(\"Train dataset composed of\", len(X_train), \"images and\", len(np.unique(y_train)), \"classes.\")\n",
    "print(\"Test dataset composed of\", len(X_test), \"images and\", len(np.unique(y_test)), \"classes.\")\n",
    "\n",
    "# Building dataset\n",
    "check_dirs(DATASET_PATH)\n",
    "delete_all_files_in_folder(DATASET_PATH)\n",
    "save_path = [TRAIN_DATASET_PATH, TEST_DATASET_PATH]\n",
    "Xs = [X_train, X_test]\n",
    "Ys = [y_train, y_test]\n",
    "for k in range(len(save_path)):\n",
    "    print((k+1),\"/\",len(save_path))\n",
    "    path = save_path[k]\n",
    "    X = Xs[k]\n",
    "    y = Ys[k]\n",
    "    for i in range(len(X)):\n",
    "        taxon = y[i]\n",
    "        source_file = X[i][\"source\"]\n",
    "        target_file = os.path.join(path, X[i][\"target\"])\n",
    "        check_dirs(target_file)\n",
    "        img = cv2.imread(source_file, cv2.IMREAD_GRAYSCALE)\n",
    "        if norm: img = match_histograms(img, ref, multichannel=False).astype(\"uint8\")\n",
    "        img = convert_to_square(img, new_size=256)\n",
    "        cv2.imwrite(target_file, img)\n",
    "        \n",
    "# Save dataset infos\n",
    "f = open(os.path.join(DATASET_PATH, 'dataset_infos.csv'), 'w')\n",
    "with f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"taxon\", \"n_images\"])\n",
    "    for taxon in taxa_dict:\n",
    "        if not taxon in eliminated_taxa:\n",
    "            writer.writerow([taxon, len(taxa_dict[taxon])])\n",
    "            \n",
    "print(\"Finished !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
