{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "INFO - First log\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from IPython.display import display\n",
    "import math, random\n",
    "import time, datetime, sys, os, shutil, logging\n",
    "import operator\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage.interpolation import shift\n",
    "import pandas as pd\n",
    "\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../utils/data_utils.ipynb\n",
    "\n",
    "check_dirs(SAVED_MODELS_ROOT)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(filename=\"info.log\"),\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('MAINLOGGER')\n",
    "logger.info(\"First log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_folds=10\n",
    "val_split=0.1\n",
    "pretrain_epochs = 3\n",
    "train_epochs = 20\n",
    "verbose = 0\n",
    "BATCH_SIZE = 32 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Data Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preproc(img):\n",
    "    # Zoom img\n",
    "    zoomed_img=cv2_clipped_zoom(img, np.random.uniform(80,120)/100)\n",
    "    # Shift img\n",
    "    ox, oy = np.random.randint(-30,30,2)/100\n",
    "    return tf.keras.preprocessing.image.random_shift(zoomed_img, ox, oy, row_axis=0, col_axis=1, channel_axis=2, fill_mode='nearest')\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                         rotation_range=180, \n",
    "                         brightness_range=[0.8,1.2], \n",
    "                         horizontal_flip=True, \n",
    "                         vertical_flip=True,\n",
    "                         fill_mode='nearest',\n",
    "                         preprocessing_function=preproc,\n",
    "                         #width_shift_range=10,\n",
    "                         #height_shift_range=10,\n",
    "                         #zoom_range=0,\n",
    "                         data_format=\"channels_last\")\n",
    "\n",
    "test_val = ImageDataGenerator(rescale=1./255.,\n",
    "                         data_format=\"channels_last\")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.,\n",
    "                         data_format=\"channels_last\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving dataset from: /mnt/nvme-storage/pfauregi/training/thumbails/atlas/dataset/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'166/166'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO - Initial dataset length: 9895- n classes: 166\n",
      "INFO - Fold: 1/10\n",
      "Balanced to 149 samples per class!\n",
      "Found 24734 validated image filenames belonging to 166 classes.\n",
      "Found 891 validated image filenames belonging to 166 classes.\n",
      "Found 990 validated image filenames belonging to 166 classes.\n",
      "INFO - Training new layers\n",
      "INFO - Training full model\n"
     ]
    }
   ],
   "source": [
    "X, y, _ = get_dataset(DATASET_PATH, ids=False)\n",
    "logger.info(\"Initial dataset length: \"+str(len(X))+\"- n classes: \"+ str(len(np.unique(y))))\n",
    "start = time.time()\n",
    "skf = StratifiedKFold(n_splits=n_folds)\n",
    "fold = 1\n",
    "folds_val_loss = []\n",
    "folds_val_acc = []\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "    logger.info(\"Fold: \"+str(fold)+\"/\"+str(n_folds))\n",
    "    fold+=1\n",
    "    \n",
    "    # PREPARING DATA\n",
    "    X_trainval, X_test = X[train_index], X[test_index]\n",
    "    y_trainval, y_test = y[train_index], y[test_index]\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size=val_split, random_state=42, stratify=y_trainval)\n",
    "    X_train, y_train, max_samples = balance_dataset(X_train, y_train, max_samples=None)\n",
    "    df_train = pd.DataFrame(np.transpose([X_train, y_train]), columns = ['X', 'y'])\n",
    "    df_val = pd.DataFrame(np.transpose([X_val, y_val]), columns = ['X', 'y'])\n",
    "    df_test = pd.DataFrame(np.transpose([X_test, y_test]), columns = ['X', 'y'])\n",
    "    \n",
    "    # PREPARING GENERATORS\n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "            dataframe=df_train,\n",
    "            x_col='X',\n",
    "            y_col='y',\n",
    "            target_size=(256, 256),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            #classes=classes_array,\n",
    "            class_mode='categorical')\n",
    "    val_generator = test_datagen.flow_from_dataframe(\n",
    "            dataframe=df_val,\n",
    "            x_col='X',\n",
    "            y_col='y',\n",
    "            target_size=(256, 256),\n",
    "            batch_size=1,\n",
    "            classes=train_generator.class_indices,\n",
    "            class_mode='categorical')\n",
    "    test_generator = test_datagen.flow_from_dataframe(\n",
    "            dataframe=df_test,\n",
    "            x_col='X',\n",
    "            y_col='y',\n",
    "            target_size=(256, 256),\n",
    "            batch_size=1,\n",
    "            classes=train_generator.class_indices,\n",
    "            class_mode='categorical')\n",
    "    train_spe = train_generator.samples // BATCH_SIZE\n",
    "    val_spe = val_generator.samples\n",
    "    test_spe = test_generator.samples\n",
    "    \n",
    "    # PREPARING MODEL\n",
    "    input_tensor = Input(shape=(256, 256, 3))\n",
    "    base_model = Xception(include_top=False, weights='imagenet', input_tensor=input_tensor, pooling=None)\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    out = Dense(len(train_generator.class_indices.keys()), activation='softmax')(x)\n",
    "    model = Model(inputs=base_model.input, outputs=out)\n",
    "    checkpointer = ModelCheckpoint(filepath=\"weights.hdf5\", verbose=verbose, save_best_only=True)\n",
    "    #optimizer = SGD(lr=0.1, decay=0.0001, momentum=0, nesterov=False)\n",
    "    optimizer = \"adam\"\n",
    "    \n",
    "    # TRAINING\n",
    "    logger.info(\"Training new layers\")\n",
    "    for layer in base_model.layers: layer.trainable = False\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(train_generator, \n",
    "              epochs=pretrain_epochs, \n",
    "              steps_per_epoch=train_spe,\n",
    "              use_multiprocessing=False, \n",
    "              validation_data=val_generator,\n",
    "              validation_steps=val_spe,\n",
    "              callbacks=[],\n",
    "              verbose=verbose,\n",
    "              initial_epoch=0)\n",
    "    logger.info(\"Training full model\")\n",
    "    for layer in model.layers: layer.trainable = True\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model.fit(train_generator, \n",
    "              epochs=train_epochs, \n",
    "              steps_per_epoch=train_spe,\n",
    "              use_multiprocessing=False, \n",
    "              validation_data=val_generator,\n",
    "              validation_steps=val_spe,\n",
    "              callbacks=[checkpointer],\n",
    "              verbose=verbose,\n",
    "              initial_epoch=5)\n",
    "    model.load_weights('weights.hdf5')\n",
    "    os.remove('weights.hdf5')\n",
    "    \n",
    "    # Getting result score\n",
    "    logger.info(\"Evaluating!\")\n",
    "    logger.info(\"[val_loss, val_accuracy]: \"+str(model.evaluate(val_generator, steps=val_spe)))\n",
    "    fold_scores = model.evaluate(test_generator, steps=test_spe)\n",
    "    logger.info(\"[test_loss, test_accuracy]: \"+str(fold_scores))\n",
    "    folds_val_loss.append(fold_scores[0])\n",
    "    folds_val_acc.append(fold_scores[1])\n",
    "cv_score = np.mean(folds_val_acc)\n",
    "cv_std = np.std(folds_val_acc)\n",
    "time_elapsed = time.time()-start\n",
    "logger.info(\"Mean accuracy: \"+str(cv_score)+\"+-\"+str(cv_std))\n",
    "logger.info(\"Computed in \"+str(time_elapsed)+\"s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
