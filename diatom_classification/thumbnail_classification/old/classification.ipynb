{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'), PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "import cv2\n",
    "from sys import getsizeof\n",
    "from IPython.display import display\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "import sys\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.xception import Xception\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import *\n",
    "#tf.config.optimizer.set_jit(True)\n",
    "#tf.debugging.set_log_device_placement(True)\n",
    "print(tf.config.experimental.list_physical_devices())\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 20\n",
    "TRAIN_P = 0.8\n",
    "VAL_P = 0.2\n",
    "TEST_P = 0\n",
    "\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../utils/data_utils.ipynb\n",
    "\n",
    "id_map = get_selected_taxons(\"../../selected_taxons.txt\")\n",
    "n_classes = len(list(id_map.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'185/185'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "files, labels = get_dataset()\n",
    "NB_SAMPLES = len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for image, label in full_dataset.take(1):\n",
    "#    print(image.numpy().shape)\n",
    "#    print(label.numpy())\n",
    "#    display(Image.fromarray(np.uint8(image.numpy()*255)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_load_image(image_path, label):\n",
    "    img_file = tf.io.read_file(image_path)\n",
    "    img = tf.image.decode_png(img_file, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32) #0-1 range\n",
    "    return img, label\n",
    "\n",
    "full_dataset = tf.data.Dataset.from_tensor_slices((files, tf.keras.utils.to_categorical(labels, num_classes=None, dtype='float32')))\n",
    "full_dataset = full_dataset.shuffle(len(files))\n",
    "full_dataset = full_dataset.map(cb_load_image, num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "##full_dataset = full_dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29578\n",
      "7394\n"
     ]
    }
   ],
   "source": [
    "DATASET_SIZE = len(files)\n",
    "train_size = int(TRAIN_P * DATASET_SIZE)\n",
    "val_size = int(VAL_P * DATASET_SIZE)\n",
    "test_size = int(TEST_P * DATASET_SIZE)\n",
    "\n",
    "train_dataset = full_dataset.take(train_size)\n",
    "tmp_dataset = full_dataset.skip(train_size)\n",
    "val_dataset = tmp_dataset.take(val_size)\n",
    "tmp_dataset = full_dataset.skip(val_size)\n",
    "test_dataset = tmp_dataset.take(test_size)\n",
    "\n",
    "print(tf.data.experimental.cardinality(train_dataset).numpy())\n",
    "print(tf.data.experimental.cardinality(val_dataset).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model desgin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fetching base model\n",
    "#base_model = Xception(include_top=False, weights='imagenet', input_shape=(256, 256, 3), pooling=None)\n",
    "input_tensor = Input(shape=(256, 256, 3))\n",
    "base_model = InceptionV3(weights='imagenet', input_tensor=input_tensor, include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting model for specifiv case\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "out = Dense(230, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting tensorboard\n",
    "!rm -rf LOG_DIR\n",
    "log_dir = LOG_DIR + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New layers only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 epochs composed of 923 batches (steps) of 32 images.\n"
     ]
    }
   ],
   "source": [
    "print(int(0.1*N_EPOCHS), \"epochs composed of\", (int(train_size/BATCH_SIZE)-1), \"batches (steps) of\", BATCH_SIZE, \"images.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 924 steps, validate for 231 steps\n",
      "Epoch 1/3\n",
      "  1/924 [..............................] - ETA: 1:32:57 - loss: 5.4720 - accuracy: 0.0312WARNING:tensorflow:Method (on_train_batch_end) is slow compared to the batch update (0.193511). Check your callbacks.\n",
      "924/924 [==============================] - 76s 83ms/step - loss: 2.8451 - accuracy: 0.3487 - val_loss: 4.2532 - val_accuracy: 0.1201\n",
      "Epoch 2/3\n",
      "924/924 [==============================] - 75s 81ms/step - loss: 1.4774 - accuracy: 0.5947 - val_loss: 4.8679 - val_accuracy: 0.1305\n",
      "Epoch 3/3\n",
      "924/924 [==============================] - 73s 79ms/step - loss: 1.1828 - accuracy: 0.6652 - val_loss: 5.4622 - val_accuracy: 0.1272\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe0654615f8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs_train1 = 3\n",
    "\n",
    "train_train1 = train_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "val_train1 = val_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_train1, \n",
    "          epochs=n_epochs_train1, \n",
    "          steps_per_epoch=int(train_size/BATCH_SIZE),\n",
    "          use_multiprocessing=True, \n",
    "          validation_data=val_train1,\n",
    "          validation_steps=int(val_size/BATCH_SIZE),\n",
    "          callbacks=[tensorboard_callback],\n",
    "          initial_epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_train1 = train_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "val_train1 = val_dataset.repeat(n_epochs_train1).batch(BATCH_SIZE)\n",
    "\n",
    "print(n_epochs_train1*int(train_size/BATCH_SIZE))\n",
    "print(tf.data.experimental.cardinality(train_train1).numpy())\n",
    "print(n_epochs_train1*int(val_size/BATCH_SIZE))\n",
    "print(int(tf.data.experimental.cardinality(val_train1).numpy()))\n",
    "val_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training 2 last inceptions blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 923 steps, validate for 230 steps\n",
      "Epoch 4/13\n",
      "923/923 [==============================] - 83s 90ms/step - loss: 0.9412 - accuracy: 0.7200 - val_loss: 3.8516 - val_accuracy: 0.2923\n",
      "Epoch 5/13\n",
      "923/923 [==============================] - 80s 87ms/step - loss: 0.4425 - accuracy: 0.8576 - val_loss: 3.9563 - val_accuracy: 0.2921\n",
      "Epoch 6/13\n",
      "923/923 [==============================] - 82s 89ms/step - loss: 0.3202 - accuracy: 0.8966 - val_loss: 4.5561 - val_accuracy: 0.2745\n",
      "Epoch 7/13\n",
      "923/923 [==============================] - 82s 88ms/step - loss: 0.2618 - accuracy: 0.9153 - val_loss: 4.7518 - val_accuracy: 0.2973\n",
      "Epoch 8/13\n",
      "923/923 [==============================] - 83s 89ms/step - loss: 0.2224 - accuracy: 0.9260 - val_loss: 5.0559 - val_accuracy: 0.2894\n",
      "Epoch 9/13\n",
      "923/923 [==============================] - 82s 89ms/step - loss: 0.1874 - accuracy: 0.9361 - val_loss: 4.1446 - val_accuracy: 0.3476\n",
      "Epoch 10/13\n",
      "923/923 [==============================] - 82s 88ms/step - loss: 0.1659 - accuracy: 0.9440 - val_loss: 3.8115 - val_accuracy: 0.3626\n",
      "Epoch 11/13\n",
      "923/923 [==============================] - 82s 89ms/step - loss: 0.1430 - accuracy: 0.9537 - val_loss: 3.5188 - val_accuracy: 0.3957\n",
      "Epoch 12/13\n",
      "923/923 [==============================] - 81s 88ms/step - loss: 0.1326 - accuracy: 0.9562 - val_loss: 3.9849 - val_accuracy: 0.3750\n",
      "Epoch 13/13\n",
      "923/923 [==============================] - 81s 88ms/step - loss: 0.1186 - accuracy: 0.9609 - val_loss: 5.1802 - val_accuracy: 0.2977\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe064d36c50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs_train2 = n_epochs_train1+10\n",
    "\n",
    "train_train2 = train_dataset.repeat(n_epochs_train2).batch(BATCH_SIZE)\n",
    "val_train2 = val_dataset.repeat(n_epochs_train2).batch(BATCH_SIZE)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_train2, \n",
    "          epochs=n_epochs_train2, \n",
    "          steps_per_epoch=int(train_size/BATCH_SIZE)-1,\n",
    "          use_multiprocessing=True, \n",
    "          validation_data=val_train2,\n",
    "          validation_steps=int(val_size/BATCH_SIZE)-1,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          initial_epoch=n_epochs_train1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 923 steps, validate for 230 steps\n",
      "Epoch 14/33\n",
      "923/923 [==============================] - 155s 168ms/step - loss: 0.7994 - accuracy: 0.7686 - val_loss: 0.5006 - val_accuracy: 0.8414\n",
      "Epoch 15/33\n",
      "923/923 [==============================] - 148s 161ms/step - loss: 0.3591 - accuracy: 0.8835 - val_loss: 0.6138 - val_accuracy: 0.8069\n",
      "Epoch 16/33\n",
      "923/923 [==============================] - 149s 161ms/step - loss: 0.2694 - accuracy: 0.9124 - val_loss: 0.2737 - val_accuracy: 0.9087\n",
      "Epoch 17/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.2250 - accuracy: 0.9268 - val_loss: 1.2448 - val_accuracy: 0.7073\n",
      "Epoch 18/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.1946 - accuracy: 0.9356 - val_loss: 0.3018 - val_accuracy: 0.9049\n",
      "Epoch 19/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.1636 - accuracy: 0.9486 - val_loss: 0.3730 - val_accuracy: 0.8852\n",
      "Epoch 20/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.1622 - accuracy: 0.9468 - val_loss: 0.2106 - val_accuracy: 0.9340\n",
      "Epoch 21/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.1395 - accuracy: 0.9551 - val_loss: 0.2097 - val_accuracy: 0.9311\n",
      "Epoch 22/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.1198 - accuracy: 0.9607 - val_loss: 0.3037 - val_accuracy: 0.9167\n",
      "Epoch 23/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.1073 - accuracy: 0.9663 - val_loss: 0.6767 - val_accuracy: 0.8292\n",
      "Epoch 24/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.1087 - accuracy: 0.9656 - val_loss: 0.1565 - val_accuracy: 0.9477\n",
      "Epoch 25/33\n",
      "923/923 [==============================] - 150s 162ms/step - loss: 0.0987 - accuracy: 0.9695 - val_loss: 0.2322 - val_accuracy: 0.9344\n",
      "Epoch 26/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.0877 - accuracy: 0.9714 - val_loss: 0.3257 - val_accuracy: 0.9179\n",
      "Epoch 27/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.0762 - accuracy: 0.9755 - val_loss: 0.0899 - val_accuracy: 0.9719\n",
      "Epoch 28/33\n",
      "923/923 [==============================] - 149s 162ms/step - loss: 0.0802 - accuracy: 0.9750 - val_loss: 0.1610 - val_accuracy: 0.9503\n",
      "Epoch 29/33\n",
      "923/923 [==============================] - 149s 161ms/step - loss: 0.0708 - accuracy: 0.9779 - val_loss: 0.1382 - val_accuracy: 0.9577\n",
      "Epoch 30/33\n",
      "923/923 [==============================] - 149s 161ms/step - loss: 0.0683 - accuracy: 0.9786 - val_loss: 0.1013 - val_accuracy: 0.9694\n",
      "Epoch 31/33\n",
      "923/923 [==============================] - 149s 161ms/step - loss: 0.0652 - accuracy: 0.9797 - val_loss: 0.1134 - val_accuracy: 0.9606\n",
      "Epoch 32/33\n",
      "923/923 [==============================] - 149s 161ms/step - loss: 0.0582 - accuracy: 0.9824 - val_loss: 0.1118 - val_accuracy: 0.9679\n",
      "Epoch 33/33\n",
      "923/923 [==============================] - 149s 161ms/step - loss: 0.0597 - accuracy: 0.9808 - val_loss: 0.0468 - val_accuracy: 0.9846\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe064f75550>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs_train3 = n_epochs_train2+20\n",
    "\n",
    "train_train3 = train_dataset.repeat(n_epochs_train3).batch(BATCH_SIZE)\n",
    "val_train3 = val_dataset.repeat(n_epochs_train3).batch(BATCH_SIZE)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_train3, \n",
    "          epochs=n_epochs_train3, \n",
    "          steps_per_epoch=int(train_size/BATCH_SIZE)-1,\n",
    "          use_multiprocessing=True, \n",
    "          validation_data=val_train3,\n",
    "          validation_steps=int(val_size/BATCH_SIZE)-1,\n",
    "          callbacks=[tensorboard_callback],\n",
    "          initial_epoch=n_epochs_train2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, SAVED_MODELS_ROOT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
