{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.1.0\n",
      "Eager execution:  True\n",
      "Listing CPUs:  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n",
      "Listing GPUs:  []\n",
      "Listing XLA_GPUs:  [PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]\n"
     ]
    }
   ],
   "source": [
    "import hashlib\n",
    "import io\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "%run ./variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "print('Tensorflow version: ', tf.__version__)\n",
    "print(\"Eager execution: \", tf.executing_eagerly())\n",
    "print(\"Listing CPUs: \", tf.config.list_physical_devices('CPU'))\n",
    "print(\"Listing GPUs: \", tf.config.list_physical_devices('GPU'))\n",
    "print(\"Listing XLA_GPUs: \", tf.config.list_physical_devices('XLA_GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(annotation, id_map, binary=True, segmentation=False, verbose=False):\n",
    "    ## LOADING IMAGE\n",
    "    img_path = os.path.join(DATA, annotation[\"img_path\"])\n",
    "    encoded_image_data, width, height = load_png(img_path)\n",
    "    key = hashlib.sha256(encoded_image_data).hexdigest()\n",
    "    \n",
    "    if verbose:\n",
    "        full_image = np.array(Image.open(img_path))\n",
    "        full_image = np.expand_dims(full_image, -1)\n",
    "        full_image = np.repeat(full_image, 3, 2)\n",
    "        full_image = np.array(full_image)\n",
    "    \n",
    "    ## GENERAL FEATURES\n",
    "    image_format = IMG_TYPE.encode('utf8') # b'jpeg' or b'png'\n",
    "    ## DEFINING BOUNDING BOXES\n",
    "    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "    xmaxs = [] # List of normalized right x coordinates in bounding box\n",
    "    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n",
    "    classes_text = [] # List of string class name of bounding box (1 per box)\n",
    "    classes = [] # List of integer class id of bounding box (1 per box)\n",
    "    labels = annotation[\"labels\"]\n",
    "    for label in labels:\n",
    "        ymins.append(label[\"xmin\"]/width)\n",
    "        ymaxs.append(label[\"xmax\"]/width)\n",
    "        xmins.append(label[\"ymin\"]/height)\n",
    "        xmaxs.append(label[\"ymax\"]/height)\n",
    "        if verbose:\n",
    "            vis_util.draw_bounding_box_on_image_array(full_image, \n",
    "                                             label[\"xmin\"]/width, \n",
    "                                             label[\"ymin\"]/height,\n",
    "                                             label[\"xmax\"]/width,\n",
    "                                             label[\"ymax\"]/height,\n",
    "                                             use_normalized_coordinates=True)\n",
    "        if binary:\n",
    "            classes_text.append(next(iter(id_map)).encode('utf8'))\n",
    "            classes.append(id_map[next(iter(id_map))])\n",
    "        else:\n",
    "            classes_text.append(label[\"taxon\"].encode('utf8'))\n",
    "            classes.append(id_map[label[\"taxon\"]])\n",
    "    \n",
    "    feature_dict = {\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs)\n",
    "    }\n",
    "    \n",
    "    if segmentation:\n",
    "        masks_list = []\n",
    "        for label in labels:\n",
    "            path = label[\"mask_path\"]\n",
    "            im = Image.open(os.path.join(DATA, path))\n",
    "            imgByteArr = io.BytesIO()\n",
    "            im.save(imgByteArr, format='PNG')\n",
    "            masks_list.append(imgByteArr.getvalue())\n",
    "            if verbose:\n",
    "                img_pil = Image.open(imgByteArr)\n",
    "                img_pil = np.array(img_pil)\n",
    "                vis_util.draw_mask_on_image_array(full_image,img_pil)\n",
    "        feature_dict['image/object/mask']=dataset_util.bytes_list_feature(masks_list)\n",
    "    \n",
    "    if verbose:\n",
    "        display(Image.fromarray(full_image))\n",
    "        \n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(annotations_path, id_map, output_path, binary=True, segmentation=False, verbose=False):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    print(\"Creating tfrecord based on \"+str(len(annotations_path))+\" example(s). Save location: \"+output_path)\n",
    "    #print(\"0 %\")\n",
    "    a = display(str(0)+\"/\"+str(len(annotations_path)),display_id=True)\n",
    "    for i, annotation_file in enumerate(annotations_path):\n",
    "        a.update(str(i+1)+\"/\"+str(len(annotations_path)))\n",
    "        annotation_path = os.path.join(DATA, ANNOTATIONS_FOLDER, annotation_file)\n",
    "        if os.path.isfile(annotation_path):\n",
    "            annotation = pickle.load(open(annotation_path, \"rb\"))\n",
    "            tf_example = create_tf_example(annotation, id_map, binary=binary, segmentation=segmentation, verbose=verbose)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "        else:\n",
    "            print(\"WARNING: Error while loading annoation \"+annotation_path+\". Ignoring image.\")\n",
    "    writer.close()\n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(binary=True, save=True):\n",
    "    if binary: pb_name = \"binary_label_map\"\n",
    "    else: pb_name = \"multiclass_label_map\"\n",
    "    id_map = pickle.load(open(os.path.join(DATA, MAPS_FOLDER, pb_name+\".pickle\"), \"rb\" ))\n",
    "    if save: create_pbtxt(id_map, OUPUT_PATH+'/'+pb_name+\".pbtxt\")\n",
    "    return id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LISTING ALL ANNOTATIONS FROM THE IMAGE FOLDER\n",
    "img_path = os.path.join(DATA, ANNOTATIONS_FOLDER)\n",
    "annotations = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(img_path):\n",
    "    for filename in filenames:\n",
    "        splitted = filename.split('.')\n",
    "        if len(splitted) and splitted[1].lower()==\"pickle\":\n",
    "            annotations.append(filename)\n",
    "random.shuffle(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating tfrecord based on 18000 example(s). Save location: /mnt/nvme-storage/pfauregi/artificial_datasets/dataset03_tfr/train_dataset.record\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'18000/18000'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "Creating tfrecord based on 2001 example(s). Save location: /mnt/nvme-storage/pfauregi/artificial_datasets/dataset03_tfr/test_dataset.record\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2001/2001'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "binary_dataset = False\n",
    "segmentation = False\n",
    "verbose = False\n",
    "# Get id_map\n",
    "id_map = get_map(binary=binary_dataset, save=True)\n",
    "# Train dataset\n",
    "train_record_path = os.path.join(OUPUT_PATH, TRAIN_OUTPUT_FILE)\n",
    "check_dirs(train_record_path)\n",
    "create_tf_record(annotations[0:int(PERCENTAGE_TRAIN*len(annotations))], \n",
    "                 id_map,\n",
    "                 train_record_path, \n",
    "                 binary=binary_dataset, \n",
    "                 segmentation=True, \n",
    "                 verbose=verbose)\n",
    "# Test dataset\n",
    "test_record_path = os.path.join(OUPUT_PATH, TEST_OUTPUT_FILE)\n",
    "check_dirs(test_record_path)\n",
    "create_tf_record(annotations[int(PERCENTAGE_TRAIN*len(annotations)):len(annotations)], \n",
    "                 id_map,\n",
    "                 test_record_path, \n",
    "                 binary=binary_dataset, \n",
    "                 segmentation=True,\n",
    "                 verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_map = get_map(binary=False, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'AAMB': 170,\n",
       " 'ACLI': 60,\n",
       " 'ADAM': 95,\n",
       " 'ADCT': 172,\n",
       " 'ADEU': 76,\n",
       " 'ADLA': 83,\n",
       " 'ADPY': 178,\n",
       " 'ADRI': 99,\n",
       " 'ADRU': 177,\n",
       " 'ADSU': 33,\n",
       " 'AFOR': 179,\n",
       " 'AMID': 48,\n",
       " 'AOVA': 65,\n",
       " 'APED': 12,\n",
       " 'AUAJ': 158,\n",
       " 'AUGA': 134,\n",
       " 'AUGR': 15,\n",
       " 'AUPU': 124,\n",
       " 'BPAX': 156,\n",
       " 'CAEX': 90,\n",
       " 'CAGR': 94,\n",
       " 'CAMB': 44,\n",
       " 'CATO': 147,\n",
       " 'CCMP': 145,\n",
       " 'CDUB': 1,\n",
       " 'CEUG': 129,\n",
       " 'CINV': 137,\n",
       " 'CLCT': 152,\n",
       " 'CLNT': 85,\n",
       " 'CMED': 132,\n",
       " 'CMEN': 117,\n",
       " 'CMLF': 136,\n",
       " 'CNLP': 42,\n",
       " 'COPL': 8,\n",
       " 'CPED': 161,\n",
       " 'CPLA': 80,\n",
       " 'CRAC': 30,\n",
       " 'CSLP': 88,\n",
       " 'CTPU': 52,\n",
       " 'CTWE': 67,\n",
       " 'DCOF': 133,\n",
       " 'DMES': 21,\n",
       " 'DMON': 18,\n",
       " 'DOCU': 26,\n",
       " 'DSTE': 139,\n",
       " 'DTEN': 9,\n",
       " 'DVUL': 66,\n",
       " 'ECAE': 150,\n",
       " 'ECPM': 41,\n",
       " 'EEXI': 173,\n",
       " 'EICD': 45,\n",
       " 'EMIN': 58,\n",
       " 'ENMI': 24,\n",
       " 'ENVE': 68,\n",
       " 'EOCO': 56,\n",
       " 'ESLE': 70,\n",
       " 'ESUM': 35,\n",
       " 'ETEN': 61,\n",
       " 'FCRO': 113,\n",
       " 'FFVI': 123,\n",
       " 'FGRA': 116,\n",
       " 'FLEN': 183,\n",
       " 'FMES': 77,\n",
       " 'FMOC': 96,\n",
       " 'FNEV': 19,\n",
       " 'FPEC': 34,\n",
       " 'FPRU': 63,\n",
       " 'FSBH': 86,\n",
       " 'FSLU': 107,\n",
       " 'FVAU': 29,\n",
       " 'GACC': 100,\n",
       " 'GBOB': 184,\n",
       " 'GCLF': 10,\n",
       " 'GCUN': 101,\n",
       " 'GELG': 114,\n",
       " 'GLAT': 78,\n",
       " 'GMIN': 92,\n",
       " 'GOLI': 54,\n",
       " 'GPAR': 74,\n",
       " 'GPLI': 40,\n",
       " 'GPRI': 91,\n",
       " 'GRHB': 20,\n",
       " 'GSCI': 169,\n",
       " 'GTER': 57,\n",
       " 'GTRU': 105,\n",
       " 'GYAT': 167,\n",
       " 'GYKU': 174,\n",
       " 'HARC': 108,\n",
       " 'HLMO': 141,\n",
       " 'HVEN': 73,\n",
       " 'KALA': 164,\n",
       " 'KCLE': 144,\n",
       " 'LGOE': 17,\n",
       " 'MAAT': 110,\n",
       " 'MCCO': 160,\n",
       " 'MCIR': 146,\n",
       " 'MVAR': 121,\n",
       " 'NAAN': 97,\n",
       " 'NACD': 140,\n",
       " 'NACI': 112,\n",
       " 'NAMP': 49,\n",
       " 'NANT': 39,\n",
       " 'NCIN': 155,\n",
       " 'NCPL': 102,\n",
       " 'NCPR': 154,\n",
       " 'NCRY': 106,\n",
       " 'NCTE': 120,\n",
       " 'NCTO': 32,\n",
       " 'NCTV': 16,\n",
       " 'NDIS': 82,\n",
       " 'NERI': 162,\n",
       " 'NESC': 37,\n",
       " 'NEXI': 111,\n",
       " 'NFIL': 5,\n",
       " 'NFON': 3,\n",
       " 'NGER': 43,\n",
       " 'NGRE': 128,\n",
       " 'NHEU': 53,\n",
       " 'NIAR': 2,\n",
       " 'NIBU': 185,\n",
       " 'NIFQ': 149,\n",
       " 'NINC': 153,\n",
       " 'NIPF': 182,\n",
       " 'NIPU': 125,\n",
       " 'NLAN': 138,\n",
       " 'NLIN': 31,\n",
       " 'NMIC': 72,\n",
       " 'NNOT': 135,\n",
       " 'NPAD': 175,\n",
       " 'NPAE': 51,\n",
       " 'NPAL': 122,\n",
       " 'NRAD': 64,\n",
       " 'NRCH': 14,\n",
       " 'NRCS': 163,\n",
       " 'NREC': 176,\n",
       " 'NRHY': 81,\n",
       " 'NROS': 130,\n",
       " 'NSIA': 103,\n",
       " 'NSOC': 4,\n",
       " 'NSTS': 23,\n",
       " 'NSUA': 157,\n",
       " 'NTPT': 46,\n",
       " 'NTRV': 47,\n",
       " 'NULA': 131,\n",
       " 'NVDA': 171,\n",
       " 'NVEN': 93,\n",
       " 'NVIR': 168,\n",
       " 'NYCO': 55,\n",
       " 'NZSU': 38,\n",
       " 'PAPR': 11,\n",
       " 'PBIO': 126,\n",
       " 'PDAO': 79,\n",
       " 'PGRN': 181,\n",
       " 'PHEL': 6,\n",
       " 'PLAU': 25,\n",
       " 'PLEV': 109,\n",
       " 'PLFR': 180,\n",
       " 'PSAT': 59,\n",
       " 'PSBR': 22,\n",
       " 'PSCA': 166,\n",
       " 'PTCO': 7,\n",
       " 'PTDE': 143,\n",
       " 'PTDU': 87,\n",
       " 'PTLA': 50,\n",
       " 'PULA': 69,\n",
       " 'RABB': 119,\n",
       " 'RSIN': 27,\n",
       " 'RUNI': 98,\n",
       " 'SBKU': 89,\n",
       " 'SBND': 84,\n",
       " 'SCON': 159,\n",
       " 'SHTE': 142,\n",
       " 'SIDE': 75,\n",
       " 'SKPO': 104,\n",
       " 'SLAC': 13,\n",
       " 'SPIN': 165,\n",
       " 'SPUP': 28,\n",
       " 'SSVE': 151,\n",
       " 'STMI': 36,\n",
       " 'TAPI': 71,\n",
       " 'TFAS': 127,\n",
       " 'TGES': 115,\n",
       " 'THLA': 118,\n",
       " 'TLEV': 62,\n",
       " 'UULN': 148}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
