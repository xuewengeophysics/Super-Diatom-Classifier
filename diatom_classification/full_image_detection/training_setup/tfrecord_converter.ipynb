{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, io, logging, os\n",
    "import pickle\n",
    "import PIL.Image\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "from IPython.display import clear_output\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from object_detection.utils import dataset_util\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "%run ../../global_variables.ipynb\n",
    "%run ../detect_variables.ipynb\n",
    "%run ./utils.ipynb\n",
    "%run ../../utils/data_utils.ipynb\n",
    "%run ../../utils/object_detection_utils.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(annotation, id_map, folder=\".\", binary=True, segmentation=False, verbose=False):\n",
    "    ## LOADING IMAGE\n",
    "    if IMAGE_FOLDER is None:\n",
    "        img_path = os.path.join(DATASET_ROOT, folder, annotation[\"folder\"], annotation[\"filename\"])\n",
    "    else:\n",
    "        img_path = os.path.join(DATASET_ROOT, folder, IMAGE_FOLDER, annotation[\"filename\"])\n",
    "    encoded_image_data, width, height = load_png(img_path)\n",
    "    key = hashlib.sha256(encoded_image_data).hexdigest()\n",
    "    filename = img_path.split(\"/\")[-1]\n",
    "    \n",
    "    if verbose:\n",
    "        full_image = np.array(Image.open(img_path))\n",
    "        full_image = np.expand_dims(full_image, -1)\n",
    "        full_image = np.repeat(full_image, 3, 2)\n",
    "        full_image = np.array(full_image)\n",
    "    \n",
    "    ## GENERAL FEATURES\n",
    "    image_format = IMG_TYPE.encode('utf8') # b'jpeg' or b'png'\n",
    "    ## DEFINING BOUNDING BOXES\n",
    "    xmins = [] # List of normalized left x coordinates in bounding box (1 per box)\n",
    "    xmaxs = [] # List of normalized right x coordinates in bounding box\n",
    "    ymins = [] # List of normalized top y coordinates in bounding box (1 per box)\n",
    "    ymaxs = [] # List of normalized bottom y coordinates in bounding box\n",
    "    classes_text = [] # List of string class name of bounding box (1 per box)\n",
    "    classes = [] # List of integer class id of bounding box (1 per box)\n",
    "    labels = annotation[\"objects\"]\n",
    "    for label in labels:\n",
    "        if FILTER is None or label[\"name\"] in FILTER:\n",
    "            ymins.append(label[\"ymin\"]/width)\n",
    "            ymaxs.append(label[\"ymax\"]/width)\n",
    "            xmins.append(label[\"xmin\"]/height)\n",
    "            xmaxs.append(label[\"xmax\"]/height)\n",
    "            if verbose:\n",
    "                vis_util.draw_bounding_box_on_image_array(full_image, \n",
    "                                                 label[\"ymin\"]/width, \n",
    "                                                 label[\"xmin\"]/height,\n",
    "                                                 label[\"ymax\"]/width,\n",
    "                                                 label[\"xmax\"]/height,\n",
    "                                                 use_normalized_coordinates=True)\n",
    "            if binary:\n",
    "                classes_text.append(next(iter(id_map)).encode('utf8'))\n",
    "                classes.append(id_map[next(iter(id_map))])\n",
    "            else:\n",
    "                classes_text.append(label[\"name\"].encode('utf8'))\n",
    "                classes.append(id_map[label[\"name\"]])\n",
    "    \n",
    "    feature_dict = {\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename.encode('utf8')),\n",
    "        'image/key/sha256': dataset_util.bytes_feature(key.encode('utf8')),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_image_data),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs)\n",
    "    }\n",
    "    \n",
    "    if segmentation:\n",
    "        masks_list = []\n",
    "        for label in labels:\n",
    "            path = label[\"mask_path\"]\n",
    "            im = Image.open(os.path.join(DATASET_ROOT, path))\n",
    "            imgByteArr = io.BytesIO()\n",
    "            im.save(imgByteArr, format='PNG')\n",
    "            masks_list.append(imgByteArr.getvalue())\n",
    "            if verbose:\n",
    "                img_pil = Image.open(imgByteArr)\n",
    "                img_pil = np.array(img_pil)\n",
    "                vis_util.draw_mask_on_image_array(full_image,img_pil)\n",
    "        feature_dict['image/object/mask']=dataset_util.bytes_list_feature(masks_list)\n",
    "    \n",
    "    if verbose:\n",
    "        display(Image.fromarray(full_image))\n",
    "        \n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_record(annotations_path, id_map, output_path, folder=\".\", binary=True, segmentation=False, verbose=False):\n",
    "    writer = tf.io.TFRecordWriter(output_path)\n",
    "    print(\"Creating tfrecord based on \"+str(len(annotations_path))+\" example(s). Save location: \"+output_path)\n",
    "    a = display(str(0)+\"/\"+str(len(annotations_path)),display_id=True)\n",
    "    for i, annotation_path in enumerate(annotations_path):\n",
    "        a.update(str(i+1)+\"/\"+str(len(annotations_path)))\n",
    "        if os.path.isfile(annotation_path):\n",
    "            annotation = parse_annotation(annotation_path)\n",
    "            tf_example = create_tf_example(annotation, id_map, folder=folder, binary=binary, segmentation=segmentation, verbose=verbose)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "        else:\n",
    "            print(\"WARNING: Error while loading annotation \"+annotation_path+\". Ignoring image.\")\n",
    "    writer.close()\n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_map(binary=True, save=True):\n",
    "    if binary: pb_name = \"binary_label_map\"\n",
    "    else: pb_name = \"multiclass_label_map\"\n",
    "    id_map = pickle.load(open(os.path.join(DATASET_ROOT, MAP_FOLDER, pb_name+\".pickle\"), \"rb\" ))\n",
    "    if save: create_pbtxt(id_map, OUPUT_PATH+'/'+pb_name+\".pbtxt\")\n",
    "    return id_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating tf record dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using validation folder!\n",
      "36001 training annotations and 4001 validation annotations!\n"
     ]
    }
   ],
   "source": [
    "# Listing annotations\n",
    "annotations_train = return_all_files_in_folder_rec(os.path.join(DATASET_ROOT, TRAIN_FOLDER, ANNOTATION_FOLDER), exts=[\"xml\"])\n",
    "random.shuffle(annotations_train)\n",
    "\n",
    "if not VAL_FOLDER is None:\n",
    "    validation_folder = VAL_FOLDER\n",
    "    print(\"Using validation folder!\")\n",
    "    annotations_val = return_all_files_in_folder_rec(os.path.join(DATASET_ROOT, VAL_FOLDER, ANNOTATION_FOLDER), exts=[\"xml\"])\n",
    "    random.shuffle(annotations_val)\n",
    "elif not SPLIT_VAL is None:\n",
    "    validation_folder = TRAIN_FOLDER\n",
    "    print(\"Splitting training dataset :\", (1-SPLIT_VAL),\"% for training/\",SPLIT_VAL,\"% for validation\")\n",
    "    annotations_train, annotations_val = train_test_split(annotations_train, test_size=SPLIT_VAL)\n",
    "else:\n",
    "    print(\"No validation.\")\n",
    "    annotations_val=[]\n",
    "print(len(annotations_train), \"training annotations and\", len(annotations_val), \"validation annotations!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting all files in /mnt/nvme-storage/pfauregi/training/obj_detection/ws_bd/dataset\n",
      "Scheme: dataset01\n",
      "Creating: /mnt/nvme-storage/pfauregi/training/obj_detection/ws_bd/dataset/train_list.csv\n",
      "Creating tfrecord based on 36001 example(s). Save location: /mnt/nvme-storage/pfauregi/training/obj_detection/ws_bd/dataset/train_dataset.record\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'36001/36001'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n",
      "Creating: /mnt/nvme-storage/pfauregi/training/obj_detection/ws_bd/dataset/test_list.csv\n",
      "Creating tfrecord based on 4001 example(s). Save location: /mnt/nvme-storage/pfauregi/training/obj_detection/ws_bd/dataset/test_dataset.record\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'4001/4001'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "check_dirs(os.path.join(OUPUT_PATH, \"init.tmp\"))\n",
    "delete_all_files_in_folder(OUPUT_PATH)\n",
    "print(\"Scheme:\", SCHEME)\n",
    "# Parameters\n",
    "verbose = False\n",
    "# Get id_map\n",
    "id_map = get_map(binary=BINARY, save=True)\n",
    "# Train dataset\n",
    "if len(annotations_train)!=0:\n",
    "    # Save csv\n",
    "    csv_train_path = os.path.join(OUPUT_PATH, \"train_list.csv\")\n",
    "    check_dirs(csv_train_path)\n",
    "    array_to_csv(annotations_train, csv_train_path)\n",
    "    # Create train tf_record\n",
    "    train_record_path = os.path.join(OUPUT_PATH, TRAIN_OUTPUT_FILE)\n",
    "    check_dirs(train_record_path)\n",
    "    create_tf_record(annotations_train, \n",
    "                     id_map,\n",
    "                     train_record_path, \n",
    "                     folder=TRAIN_FOLDER,\n",
    "                     binary=BINARY, \n",
    "                     segmentation=SEGMENTATION, \n",
    "                     verbose=verbose)\n",
    "# Test dataset\n",
    "if len(annotations_val)!=0:\n",
    "    \n",
    "    # Save csv\n",
    "    csv_test_path = os.path.join(OUPUT_PATH, \"test_list.csv\")\n",
    "    check_dirs(csv_test_path)\n",
    "    array_to_csv(annotations_val, csv_test_path)\n",
    "    # Create test tf_record\n",
    "    test_record_path = os.path.join(OUPUT_PATH, TEST_OUTPUT_FILE)\n",
    "    check_dirs(test_record_path)\n",
    "    create_tf_record(annotations_val, \n",
    "                     id_map,\n",
    "                     test_record_path, \n",
    "                     folder=validation_folder,\n",
    "                     binary=BINARY, \n",
    "                     segmentation=SEGMENTATION,\n",
    "                     verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
